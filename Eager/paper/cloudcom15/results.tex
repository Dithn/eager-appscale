In this section we describe some of the experiments we have conducted using Cerebro in the Google
App Engine public cloud environment. These experiments are aimed at estimating the regularity at
which Cerebro requires API consumers to renegotiate the auto-generated performance SLAs. That is,
we plan to assess the number of times an API consumer will be prompted to renegotiate a
SLA due to the dynamic performance changes that occur in the cloud platform, and the time duration
between these forced renegotiation events.

We deploy Cerebro's cloud SDK monitoring agent in the Google App Engine cloud, and let it benchmark
the cloud SDK operations every 60 seconds for a long period (~112 days). Then we use Cerebro
to make 95th percentile SLA predictions for the following group of five open source web applications. 

\begin{description}
\item[StudentInfo] RESTful (JAX-RS) application for managing
students of a class (adding, removing, and listing student information).
\item[ServerHealth] Monitors, computes, and reports statistics for server
uptime for a given web URL.
\item[SocialMapper] A simple social networking application with APIs for
adding users and comments.
\item[StockTrader] A stock trading application that
provides APIs for adding users, registering companies, buying and selling
stocks among users. 
\item[Rooms] A hotel booking application with APIs
for registering hotels and querying available rooms.
\end{description}

Cerebro analyzes the benchmarking results collected
by the cloud SDK monitor, and generates sequences of SLA predictions for the web APIs in the above
applications. Each prediction sequence
is a time series that spans the duration in which the cloud SDK monitor was active
in the cloud. Each prediction is timestamped. Therefore given any timestamp that falls within the
112 day period of the experiment, we can find a SLA prediction that is close to it. Further, each prediction
is associated with an integer value which indicates the consecutive number of SLA violations that should be
observed, before we may consider the prediction as obsolete.

We also compute the actual web API response times for the above five applications. This is done
by simply summing up the benchmarking data gathered by the cloud SDK monitor. For example,
consider a web API that executes the cloud SDK operations $O_{1}$, $O_{2}$ and $O_{1}$ in that order. 
Now suppose the cloud SDK monitor has gathered following benchmarking results for $O_{1}$ and
$O_{2}$:

\begin{itemize}
\item $O_{1}$: [$t_{1}:x_{1}$, $t_{2}:x_{2}$, $t_{3}:x_{3}$...]
\item $O_{2}$: [$t_{1}:y_{1}$, $t_{2}:y_{2}$, $t_{3}:y_{3}$...]
\end{itemize}

Here $t_{i}$ are timestamps at which the benchmark operations were performed. $x_{i}$ and $y_{i}$ are
execution times of the two SDK operations measured in milliseconds. Given this benchmarking data,
we can calculate the time series of actual response time of the API as follows:

[$t_{1}:2x_{1}+y_{1}$, $t_{2}:2x_{2}+y_{2}$, $t_{3}:2x_{3}+y_{3}$...]

The coefficient $2$ that appears before each $x_{i}$ term accounts for the fact that our web API
invokes the cloud SDK operation $O_{1}$ twice. In this manner, we can combine the static analysis
results of Cerebro with the cloud SDK benchmarking data to obtain a time series of estimated
actual response times for all web APIs in our sample applications.

