In this section we describe the experimental methodology we use to evaluate how the SLAs
generated by Cerebro change over time. Our goal is to understand the frequency with
which Cerebro requires API consumers to renegotiate auto-generated performance SLAs. That is,
we assess the number of times an API consumer will be prompted to renegotiate an
SLA due to the changes that occur in the cloud platform, and the time duration
between these renegotiation events.

To enable this, we deploy Cerebro's cloud 
SDK monitoring agent in the Google App Engine cloud and benchmark
the cloud SDK operations every 60 seconds for 112 days. We then use Cerebro
to make SLA predictions (95th percentile) for the following set of open source web applications. 

\begin{itemize}
\vspace{-0.05in}
\item \textit{StudentInfo}: RESTful (JAX-RS) application for managing
students of a class (adding, removing, and listing student information).
\vspace{-0.05in}
\item \textit{ServerHealth}: Monitors, computes, and reports statistics for server
uptime for a given web URL.
\vspace{-0.05in}
\item \textit{StockTrader}: A stock trading application that
provides APIs for adding users, registering companies, buying and selling
stocks among users. 
\vspace{-0.05in}
\item \textit{Rooms}: A hotel booking application with APIs
for registering hotels and querying available rooms.
\vspace{-0.05in}
\end{itemize}

Cerebro analyzes the benchmarking results collected
by the cloud SDK monitor and generates sequences of SLA predictions for the web APIs of each
application. Each prediction sequence
is a time series that spans the duration in which the cloud SDK monitor was active
in the cloud. Each prediction is timestamped. Therefore given any timestamp that falls within the
112 day period of the experiment, we can find an SLA prediction that is closest to it. 
Further, we associate each prediction with an integer value ($C_{w}$) which indicates the consecutive 
number of SLA violations that should be
observed, before we may consider the prediction to be invalid. 
%In this work we 
%use a probability of $0.00012$ to determine a rare event.  That is,
%consecutive measurements above the predicted bounds $C_{w}$
%must occur with probability $0.00012$ or lower due to random chance.

%consider three
%consecutive violations to indicate a platform change. That is,
%using the $95^{th}$ percentile, the probability of seeing $3$
%values in a row larger than the predicted percentile due to random chance
%is $(0.05)^3 = 0.00012$.

%The web API's response time for each of the four applications is computed as
%the sum of the response time of cloud SDK operations along the longest path
%through the application (the time devoted to the operations executed by the
%application is negligible by
%comparison). 
We also estimate the actual web API response times for the above four applications. 
This is done by simply summing up the benchmarking data gathered by the cloud 
SDK monitor. Again, we assume that the time spent on non cloud SDK operations
is negligible. For example,
consider a web API that executes the cloud SDK operations $O_{1}$, $O_{2}$ and $O_{1}$ in that order. 
Now suppose the cloud SDK monitor has gathered following benchmarking results for $O_{1}$ and
$O_{2}$:

\begin{itemize}
\item $O_{1}$: [$t_{1}:x_{1}$, $t_{2}:x_{2}$, $t_{3}:x_{3}$...]
\item $O_{2}$: [$t_{1}:y_{1}$, $t_{2}:y_{2}$, $t_{3}:y_{3}$...]
\end{itemize}

Here $t_{i}$ are timestamps at which the benchmark operations were performed. $x_{i}$ and $y_{i}$ are
execution times of the two SDK operations measured in milliseconds. Given this benchmarking data,
we can calculate the time series of actual response time of the API as follows:

[$t_{1}:2x_{1}+y_{1}$, $t_{2}:2x_{2}+y_{2}$, $t_{3}:2x_{3}+y_{3}$...]

The coefficient $2$ that appears with each $x_{i}$ term accounts for the fact that our web API
invokes $O_{1}$ twice. In this manner, we can combine the static analysis
results of Cerebro with the cloud SDK benchmarking data to obtain a time series of estimated
actual response times for all web APIs in our sample applications.

Having obtained a time series of SLA predictions ($T_{p}$) and a time series of actual response 
times ($T_{a}$) for each web API, we perform the following computation. From $T_{p}$ we pick a
pair $<s_{0},t_{0}>$, where $s_{0}$ is a predicted SLA value and $t_{0}$ is the timestamp associated with it. 
Then starting from $t_{0}$, we scan the time series $T_{a}$ to detect the earliest point in time
at which we can consider the predicted SLA value $s_{0}$ as invalid. This is done by comparing $s_{0}$
against each entry in $T_{a}$ that has a timestamp greater than or equal to $t_{0}$, until we see $C_{w}$ 
consecutive entries that are larger than $s_{0}$. Here $C_{w}$ is the rare event threshold 
computed by Cerebro when making SLA predictions. Having found such an SLA invalidation
event at time $t^{\prime}$, we record the duration $t^{\prime} - t_{0}$ (i.e. the SLA validity period), and 
increment the counter $invalidations$, 
which starts from $0$. Then we pick the pair $<s_{1},t_{1}>$ from $T_{p}$ where $t_{1}$ is the smallest
timestamp greater than or equal to $t^{\prime}$, and $s_{1}$ is the predicted SLA value at that timestamp.
Then we scan $T_{a}$ starting from $t_{1}$, until we detect the next SLA invalidation (for $s_{1})$. 
We repeat this process
until we exhaust either $T_{p}$ or $T_{a}$. At the end of this computation we have a distribution of SLA
validity periods, and the counter $invalidations$ indicates the number of SLA invalidations we encountered
in the process.

This experimental process simulates how a single API consumer (re-)negotiates SLAs.
Selecting the first pair of values $<s_{0},t_{0}>$ represents
the API consumer negotiating the SLA for the first time (i.e. at API subscription). 
When this SLA becomes invalid, the API consumer
renegotiates for a new SLA, which is represented by the selection of the pair $<s_{1},t_{1}>$. 
Therefore, when the simulation reaches the end of the time series, we can determine how many times the
API consumer had to renegotiate the SLA (given by $invalidations$). The
recorded SLA validity periods give an indication of the time between these renegotiation events.

For a given web API we perform the above simulation many times, using each entry in $T_{p}$ as
a starting point. That is, in each run we change our selection of $<s_{0},t_{0}>$ to be a different
entry in $T_{p}$. This way, for a time series comprised of $n$ entries, we can run the simulation 
$n-1$ times, discarding the last entry. We can assume that each simulation run corresponds to a different API
consumer. Therefore, at the end of a complete execution of the experiment we have the SLA
renegotiation counts for many different API consumers, and the empirical SLA validity period distributions 
for each of them. 

The smallest $n$ we encountered in all our experiments was 125805. That is, we
repeatedly simulated each web API SLA trace 
for at least 125804 API consumers. Similarly, the largest number of API consumers 
we performed the simulation for is 145130.
