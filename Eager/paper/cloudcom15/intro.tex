Web services today are an essential technology for implementing
complex distributed applications.  To promote modularity and reuse, 
and to leverage the testing, deployment and scaling support, and maintenance 
provided by others, developers increasingly integrate remote, Internet-accessible web services 
via web application programming interfaces (web APIs)
into their web, cloud, and mobile applications.  
The benefits to programmer productivity that such development practices
permit have resulted in the explosive growth of web API availability~\cite{pweb}.

However, reusing existing services also poses several challenges. In particular, 
web APIs can impact the performance, behavior, and reliability of the applications
that integrate them.  Moreover, web service functionality can change over time 
(even when the API remains the same), unbeknownst to the developers that use them.
Unfortunately, there is a shortage of tools that help developers 
reason about web API dependencies throughout an application's 
lifecycle (i.e. development, deployment, and runtime).  Without such tools, 
programmers must resort to extensive and continuous testing and profiling 
to understand the impact of their use of web APIs on the performance and behavior
of their applications.

To address this need, we have developed Cerebro~\cite{cerebro_paper}.
Cerebro is a cloud platform-as-a-service (PaaS) technology that
predicts the response time of web APIs hosted by the PaaS.
Cerebro enables the PaaS administrator to determine what response time service level 
agreement (SLA) can be fulfilled by each web API operation of the PaaS-hosted applications.
To enable this, Cerebro combines static analysis of each operation with 
dynamic analysis (statistical distributions) 
of the cloud platform services that each uses for its implementation 
(e.g. for persistent storage, data mangement, caching, etc.).
Cerebro uses this information to predict an upper bound for the response time 
of the application.  Cerebro is fast, does not require modification,
testing, or profiling of the application itself, and can be used to predict 
performance SLAs for a web API when it is \textit{deployed} to a PaaS cloud.
In this prior work, we showed that the response time SLAs predicted by Cerebro are
(i) are accurate -- they predict upper bounds that exceed the actual response time of the APIs
with a specific, configurable success rate, and (ii)
they are tight -- the predict upper bounds that do not exceed the actual 
response time of the APIs by a large 
margin (i.e. the predicted values are sufficiently close to the actual values). 

In this paper, we focus on the \textit{duration} of Cerebro predictions, i.e. the period
of time that predicted response time SLAs remain valid.  Predictions are invalidated
when conditions in the the platform change in ways that violate the SLA (beyond random
chance).  Such changes can result from
congestion (multitenency), resource availability, and modifications to the implementations
of PaaS services (abstracted by the cloud SDK calls). Invalidations require that 
SLAs be renegotiated and thus impose overhead on developers.  

In Cerebro, we refer to such developers as API consumers.  Cerebro gives each API consumer an initial 
response time SLA as part of application deployment to the PaaS.
The Cerebro monitoring agent tracks of performance of the PaaS services continuously.
When Cerebro detects that a change has occurred in the PaaS that
violates a previously negotiated SLA, Cerebro contacts the API consumer and renegotiates 
a new SLA for his/her application.

In this work, we use a combination of empirical testing and simulations to analyze how 
the SLA values predicted by Cerebro change over time, and how they impact API consumers. We
conduct our experiments using the Google App Engine~\cite{gae} public cloud PaaS 
using a set of open source Java web applications. Our results indicate that on average, the
SLAs predicted by Cerebro do not require renegotiation for at least 12 days. We also find that
over a period of 112 days, the maximum number of times that any API consumer must renegotiate
their SLA is six.  We also find that occasionally Cerebro prompts an API consumer to
renegotiate an SLA when the predicted new SLA value is very close to the invalidated SLA value. 
Such renegotiations are not useful and only serve to increase the renegotiation overhead
for API consumers. We thus also propose a threshold-based mechanism to minimize such cases.  
We show that doing so, we are able to further elongates SLA validity periods and 
reduce the number of renegotiations.
