Today, web services are an essential technology for implementing
complex distributed applications. They promote modularity and software reuse,
while leveraging the scalability features and maintenance 
provided by others. For these reasons developers increasingly integrate remote, 
Internet-accessible web services via web application programming interfaces (web APIs)
into their web, cloud, and mobile applications.  
The benefits to programmer productivity that such development practices
permit have resulted in a vast number and diversity of available web APIs~\cite{pweb}.

Unfortunately, reusing existing services has its drawbacks. In particular, 
web APIs impact the performance, behavior, and reliability of the applications
that integrate them.  Web service functionality and performance can change over time 
without prior notice, while their APIs remain unchanged.
Moreover, there is a shortage of tools to help developers 
reason about the impact of web API dependencies, and their potential for
dynamic change throughout an application's 
lifecycle (i.e. development, deployment, and runtime).  

In this paper, we study the use of on-line monitoring and statistical
forecasting of web API performance as a means of providing each API
consumer with a guaranteed minimum performance level.
More specifically, we explore the idea of formulating Service Level Agreements (SLAs)
for web APIs, based on automatic prediction of web API response times.
Most cloud platforms which are used to serve web APIs today only offer probabilistic
SLAs for service availability but not response time. 
Our work focuses on response time SLAs for web APIs deployed
in Platform-as-a-Service (PaaS) clouds.

We use the term ``SLA'' to refer to the minimum service level promised by the
service provider regarding some non-functional property of the service such as
its availability or performance (response time). Such SLAs are explicitly
stated by the service provider, and are associated with a correctness probability,
which can be described as the likelihood the service will meet the promised
minimum service level. An availability SLA that follows this notion
takes the form: ``the service will be available $p\%$ of the time''.
Here the value $p\%$ is the correctness probability of the SLA. Similarly, a
response time SLA would take the form of the statement: ``the service will respond 
under $Q$ milliseconds, $p\%$ of the time. Naturally, $p$ should be a value close
to $100$, for this type of SLAs to be useful in practice. 

In a corporate setting, an 
SLA would consist of additional clauses describing what happens if and when the
service fails to meet the promised minimum service level (for example, if the
service is only available $p^\prime\%$ of the time, where $p^\prime < p$). This
typically boils down to service provider paying some penalty (a refund), or 
providing some form of free service credits for the users. We do not consider such
legal and social obligations of an SLA in this work, and simply focus on the
minimum service levels and the associated correctness probabilities, since those
are the things that matter from a performance and capacity planning point of view
of an application. Some authors use the term Service Level Objective (SLO) to
refer to promised minimum service levels, and reserve the term SLA for the
aggregate of SLOs and other action clauses~\cite{Keller:2003:WFS:635430.635442}.
Since we are only looking at the minimum service levels, we disregard this
separation, and make liberal use of the term SLA.

%Without such tools, 
%programmers must resort to extensive and continuous testing and 
%profiling 
%to understand the impact of their use of web APIs on the performance 
%and behavior
%of their applications.

To enable our study, we have developed Cerebro~\cite{Jayathilaka:2015:RTS:2806777.2806842},
a system that
predicts the response time of web APIs hosted in a PaaS cloud. Cerebro is able to
determine automatically the bounds on API response time with a specific
correctness probability.
Our previous work~\cite{Jayathilaka:2015:RTS:2806777.2806842} describes the effectiveness of
Cerebro when it is used in conjunction with Google's public PaaS (Google
App Engine) and the AppScale~\cite{6488671} private on-premise PaaS.  

In this work, we
explore the use of Cerebro predictions as the basis of SLAs between an API
consumer, or client,
and a service hosted by the PaaS.  Specifically, we detail the duration
over which SLAs offered to clients of applications running in Google App
Engine persist. The SLAs based on predicted API response times do not remain 
valid indefinitely due to the
dynamic performance variations that occur in the underlying cloud platform.
Empirical analysis on cloud workload traces has shown that production cloud
platforms often display performance variations, sometimes with temporal
patterns~\cite{5948601}.

The resulting SLA duration is an important parameter because PaaS API consumers
often
wish to contract for specific minimum service level guarantees, and must
renegotiate when those guarantees expire or can no longer be sustained.  
This work demonstrates that, using a combination of on-line benchmarking
and static program analysis, Cerebro can generate SLAs that are
\textit{durable} over long periods.  That is, using Cerebro it is possible for
a user of Google App Engine or AppScale to offer statistically
reliable response time SLAs on web APIs
that persist over long periods (and thus do not require frequent
renegotiation).

%Cerebro enables the PaaS administrator to determine response time service level 
%agreements (SLAs) that applications hosted in the PaaS can guarantee.  Cerebro is able to 
%do so since PaaS applications spend a majority of their time accessing PaaS services
%(e.g. for persistent storage, data mangement, caching, etc.). Specifically, 
%Cerebro uses static analysis to extract the series of calls to PaaS services
%in an operation and uses them to estimate an upper bound on the execution time of 
%an operation.  Cerebro does so via statistical distributions of the performance of PaaS services
%in the platform that it collects via lightweight, application-independent monitoring.
%In this prior work, we find that Cerebro is fast, does not require modification,
%testing, or profiling of the applications themselves, and can be used to predict 
%performance SLAs for web APIs at \textit{deployment time}.
%We also show that Cerebro response time SLAs 
%(i) are accurate -- they predict upper bounds that exceed the actual response time of the APIs
%with a specific, configurable success rate, and (ii)
%they are tight -- they predict upper bounds that do not exceed the actual 
%response time of the APIs by a large 
%margin (i.e. the predicted values are sufficiently close to the actual values). 
%In this paper, we focus on the \textit{duration} that Cerebro predictions hold, i.e. the period
%of time that predicted response time SLAs remain valid.  Predictions are invalidated
%when conditions in the the platform change in ways that violate the SLA (beyond random
%chance).  Such changes can result from congestion (multitenency), resource availability, 
%and modifications to the implementations of the underlying PaaS services. Invalidations 
%require that SLAs be renegotiated and thus impose overhead on developers that use the 
%web APIs exported by applications hosted by the PaaS.

%We refer to such developers as API consumers.  
%Specifically, Cerebro gives each API consumer an initial 
%response time SLA when they register for an API key to use an application exported by the PaaS.
%As mentioned above, a Cerebro monitoring agent tracks the performance of the PaaS services 
%continuously.  When Cerebro detects that a change has occurred in the PaaS that
%violates a previously negotiated SLA, Cerebro contacts the API consumer
%for which the SLA is violated and renegotiates a new SLA.

To analyze SLA durability and how API consumers are 
impacted by it, we perform extensive testing and empirical evaluation
of Google App Engine using a set of open source Java web applications.  
We also employ simulation to explore different options for SLA renegotiation.
Our results indicate that on average, the minimum duration for which Cerebro SLAs 
remain valid for this PaaS is 12 days. We also show that
over a period of 112 days, the maximum number of times that any API consumer must renegotiate
their SLA is 6.  Furthermore, we find that in some cases Cerebro prompts an API consumer to
renegotiate an SLA when the predicted new SLA value is very close to the invalidated SLA value. 
Such renegotiations are not useful in practice, and only serve to increase the renegotiation overhead
for API consumers. We thus also present a threshold-based mechanism that both reduces 
the number of required renegotiations, and extends SLA validity duration.
To our knowledge, no other research or system is able to predict
durable performance SLAs for applications hosted on a public PaaS such as Google App Engine.