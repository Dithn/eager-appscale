Today, web services are an essential technology for implementing
complex distributed applications. They promote modularity and software reuse,
while leveraging the scalability features and maintenance 
provided by others. For these reasons developers increasingly integrate remote, 
Internet-accessible web services via web application programming interfaces (web APIs)
into their web, cloud, and mobile applications.  
The benefits to programmer productivity that such development practices
permit have resulted in a vast number and diversity of available web APIs~\cite{pweb}.

Unfortunately, reusing existing services has its drawbacks. In particular, 
web APIs can impact the performance, behavior, and reliability of the applications
that integrate them.  Web service functionality and performance can change over time 
without prior notice, even when the API remains the same.
Moreover, there is a shortage of tools that help developers 
reason about the impact of web API dependencies, and their potential for
dynamic change throughout an application's 
lifecycle (i.e. development, deployment, and runtime).  

In this paper, we study the use of on-line monitoring and statistical
forecasting of web API performance as a means of providing each API
consumer with a guaranteed minimum performance level. 
More specifically, we explore the idea of using web API response time forecasts
as the basis for formulating Service Level Agreements (SLAs)
for web APIs. Most cloud platforms today offer probabilistic
SLAs for service availability but not response time. 
Our work focuses on response time SLAs for web APIs deployed
in Platform-as-a-Service (PaaS) clouds.

%Without such tools, 
%programmers must resort to extensive and continuous testing and 
%profiling 
%to understand the impact of their use of web APIs on the performance 
%and behavior
%of their applications.

To enable this, we have developed Cerebro~\cite{cerebro-soccsub15},
a system that
predicts the response time of web APIs hosted in a PaaS cloud. Cerebro is able to
determine automatically the bounds on API response time with a specific
correctness probability.
Our previous work~\cite{cerebro-soccsub15} describes the effectiveness of
Cerebro when it is used with Google's public PaaS (Google
App Engine) and the AppScale~\cite{6488671} private on-premise PaaS.  

In this work, we
explore the use of Cerebro predictions as the basis of SLAs between an API
consumer, or client,
and a service hosted by the PaaS.  Specifically, we detail the duration
over which SLAs offered to clients of applications running in Google App
Engine persist. The SLA duration is important because PaaS API consumers
often
wish to contract for specific minimum guaranteed service levels, and must
renegotiate when those guarantees expire or can no longer be sustained.  
This work demonstrates that, using a combination of on-line API benchmarking
and static program analysis, Cerebro can generate SLAs that are
\textit{durable} over long periods.  That is, using Cerebro it is possible for
a user of Google App Engine or AppScale to offer statistically
reliable response time SLAs on web APIs
that persist over long periods (and thus do not require frequent
renegotiation).

%Cerebro enables the PaaS administrator to determine response time service level 
%agreements (SLAs) that applications hosted in the PaaS can guarantee.  Cerebro is able to 
%do so since PaaS applications spend a majority of their time accessing PaaS services
%(e.g. for persistent storage, data mangement, caching, etc.). Specifically, 
%Cerebro uses static analysis to extract the series of calls to PaaS services
%in an operation and uses them to estimate an upper bound on the execution time of 
%an operation.  Cerebro does so via statistical distributions of the performance of PaaS services
%in the platform that it collects via lightweight, application-independent monitoring.
%In this prior work, we find that Cerebro is fast, does not require modification,
%testing, or profiling of the applications themselves, and can be used to predict 
%performance SLAs for web APIs at \textit{deployment time}.
%We also show that Cerebro response time SLAs 
%(i) are accurate -- they predict upper bounds that exceed the actual response time of the APIs
%with a specific, configurable success rate, and (ii)
%they are tight -- they predict upper bounds that do not exceed the actual 
%response time of the APIs by a large 
%margin (i.e. the predicted values are sufficiently close to the actual values). 
%In this paper, we focus on the \textit{duration} that Cerebro predictions hold, i.e. the period
%of time that predicted response time SLAs remain valid.  Predictions are invalidated
%when conditions in the the platform change in ways that violate the SLA (beyond random
%chance).  Such changes can result from congestion (multitenency), resource availability, 
%and modifications to the implementations of the underlying PaaS services. Invalidations 
%require that SLAs be renegotiated and thus impose overhead on developers that use the 
%web APIs exported by applications hosted by the PaaS.

%We refer to such developers as API consumers.  
%Specifically, Cerebro gives each API consumer an initial 
%response time SLA when they register for an API key to use an application exported by the PaaS.
%As mentioned above, a Cerebro monitoring agent tracks the performance of the PaaS services 
%continuously.  When Cerebro detects that a change has occurred in the PaaS that
%violates a previously negotiated SLA, Cerebro contacts the API consumer
%for which the SLA is violated and renegotiates a new SLA.

To analyze SLA durability and how API consumers are 
impacted by it, we perform extensive testing and empirical evaluation
of Google App Engine using a set of open source Java web applications.  
We also employ simulation to explore different options for SLA renegotiation.
Our results indicate that on average, the minimum duration for which Cerebro SLAs 
remain valid for this PaaS is 12 days. We also show that
over a period of 112 days, the maximum number of times that any API consumer must renegotiate
their SLA is 6.  Furthermore, we find that in some cases Cerebro prompts an API consumer to
renegotiate an SLA when the predicted new SLA value is very close to the invalidated SLA value. 
Such renegotiations are not useful in practice and only serve to increase the renegotiation overhead
for API consumers. We thus also present a threshold-based mechanism that both reduces 
the number of required renegotiations and extends SLA validity duration.
To our knowledge, no other research or system is able to predict
durable performance SLAs for a public PaaS such as Google App Engine.