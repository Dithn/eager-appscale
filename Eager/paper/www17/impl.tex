We implemented a Roots prototype for the AppScale cloud platform. AppScale is an open
source PaaS cloud, API compatible with the popular Google App Engine (GAE) cloud platform.
Due to the API compatibility, any application developed for GAE can be deployed
and executed on AppScale with no code changes. Since AppScale is open source software, we
were able to modify parts of its implementation to integrate the Roots APM into it. AppScale also
turned out to be a great test environment since we could deploy it locally, and run a number of
existing App Engine applications on it.

We use ElasticSearch as the data storage component of our prototype. ElasticSearch is ideal 
for storing large volumes of structured and semi-structured data. It supports scalability and 
high availability via sharding and replication.
ElasticSearch continuously organizes and indexes data, making the information available 
for fast retrieval and efficient querying. Additionally it also provides
powerful data filtering and aggregation features, which greatly simplify the implementations of high-level
data analysis algorithms.

We configure AppScale's front-end server (based on Nginx) to tag all incoming application requests
with a unique identifier. This identifier is attached to the request as a custom HTTP header.
All data collecting agents in the cloud extract this identifier, and include it as an attribute
in all the events reported to ElasticSearch. This enables our prototype to aggregate events originating
from the same application.

We implement a number of data collecting agents in AppScale to gather runtime information
from all major components. These agents buffer data locally, and store them in ElasticSearch
in batches. For scraping server logs and storing the extracted entries in ElasticSearch,
we use the Logstash tool. Logstash supports scraping a wide range of standard log formats (e.g. 
Apache HTTPD access logs), and other custom log formats can be supported via a simple configuration.
It also integrates naturally with ElasticSearch.
To capture the PaaS kernel invocation data, we augment AppScale's PaaS SDK implementation,
which is derived from the GAE PaaS SDK. More specifically we implement an agent that records
all PaaS SDK calls, and reports them to ElasticSearch asynchronously. 

Roots pods are implemented as standalone Java server processes. Threads are used to run benchmarkers,
anomaly detectors and handlers concurrently within each pod. Pods communicate with ElasticSearch via
REST calls, and many of the data analysis tasks such as filtering and aggregation are performed
in ElasticSearch itself. By doing so a lot of the heavy computations are offloaded to the 
ElasticSearch cluster, which is specifically designed for high-performance query processing
and analytics. Some of the more sophisticated statistical analysis tasks (e.g. change point detection, 
linear regression) are implemented in R language,
and the Java-based Roots pods integrate with R using the Rserve protocol.

\subsection{SLO-based Anomaly Detector}
We implement a performance SLO checker as the primary anomaly detector in Roots. This anomaly detector
allows application developers to specify simple performance SLOs for deployed applications. A
performance SLO consists of an upper bound on the application response time ($T$), and the probability ($p$)
that the application response time falls under the specified upper bound. Therefore, a general performance 
SLO can be stated as: ``application responds under $T$ milliseconds $p$\% of the time''.

When activated for a given application this anomaly detector starts a benchmarking process
that periodically measures the response time of the target application. The detector then periodically
analyzes the collected response time measurements to check if the application meets the specified performance
SLO. Whenever it detects that the application has failed to meet the SLO, it triggers an anomaly event. It would also
purge any past response time measurements (up to the anomaly) from the memory 
when this happens.

The SLO-based anomaly detector supports following configuration parameters:
\begin{itemize}
\item Performance SLO: Response time upper bound ($T$), and the probability ($p$).
\item Sampling rate: Rate at which the target application is benchmarked.
\item Analysis rate: Rate at which the anomaly detector checks whether the application has failed to meet the SLO.
\item Minimum samples: Minimum number of samples to collect before checking for SLO violations.
\item Window size: Length of the sliding window (in time) to consider when checking for SLO violations. This
acts as a limit on the number of samples to keep in memory.
\end{itemize}

Since the detector purges past measurements when an anomaly is detected, it is not able to
check for further SLO violations until the minimum number of samples is collected. Therefore,
each anomaly is followed by a ``warm up'' period. With a sampling rate of 15 seconds, and a minimum
samples count of 100, the warm up period can last up to 25 minutes. The detector cannot find new
anomalies during this period. However, it prevents the same anomaly from being needlessly
reported multiple times.

\subsection{Path Distribution Analyzer}
Path distribution analyzer is another special anomaly detector we implement in Roots. This
anomaly detector periodically analyzes the PaaS kernel invocations made by the applications.
By aggregating the PaaS kernel invocations by application request identifiers, and then sorting them by
their sequence numbers, this anomaly detector is able to identify the sequence of
PaaS kernel invocations made by each application request. 
Each identified invocation sequence corresponds to a path of
execution through the application code (i.e. a path through the control flow graph of the application). 
Then the anomaly detector evaluates the number of requests
that invoked the same PaaS kernel invocation sequence. From that the anomaly detector
computes the distribution of different execution paths of an application.

A path distribution is comprised of the set of execution paths available in an application, along
with the proportion of requests that executed each path.
It is an indicator of the type of request workload handled by an application.
For example consider a data management application that has a read-only execution path, and a read-write 
execution path. If 90\% of the requests execute the read-only path, and the remaining 10\% of the requests
execute the read-write path, we may characterize the request workload as mostly read-only. 
Roots path distribution analyzer facilitates computing the path distribution for each application
with no static analysis or runtime instrumentation on the application code.

Roots path distribution analyzer periodically computes the path distribution for a given application.
If it detects that the latest path distribution is significantly different from the distributions seen in the 
past, it triggers an anomaly. This is done by computing the mean request proportion for each path
(over a sliding window of historical data),
and then comparing the latest request proportion values against the means. If the latest proportion
is off by more than $n$ standard deviations from its mean, the detector considers it to be an
anomaly. The sensitivity of the detector can be configured by changing the value of $n$, which
defaults to 2. 

This anomaly detector enables developers to know when the nature of their application request
workload changes. For example in the previous data management application, if suddenly 90\%
of the requests start executing the read-write path, the Roots path distribution analyzer will
detect the change as an anomaly. Similarly it is also able to detect when new paths of execution
are being invoked by requests (a form of novelty detection).

\subsection{Workload Change Analyzer}
Performance anomalies can arise due to bottlenecks in the cloud platform or changes in the application
workload.
When Roots detects a performance anomaly (e.g. an application failing to meet its performance SLO),
we need to be able to determine which of the above two causes may be behind it.
To check if the workload of an application has changed recently, Roots uses a workload change analyzer.
This is implemented as an anomaly handler, which gets executed every time an anomaly detector
notifies of a performance anomaly. Note that this is different from the path distribution analyzer,
which is implemented as an anomaly detector. While the path distribution analyzer looks for changes in the
\textit{type} of the workload, the workload change analyzer looks for changes in the workload \textit{size}.
In other words, it determines if the target application has received more requests than usual, which
may have caused a performance degradation.

Workload change analyzer uses change point detection algorithms to analyze the historical trend of 
the application workload. We use the ``number of requests
per unit time'' as the metric of workload size. This information can be obtained from the Roots
data storage as a time series. Our implementation of Roots supports a number of well known change point
detection algorithms (PELT, binary segmentation and CL method), any of which can be used to detect level shifts in the
workload size. These algorithms favor long lasting shifts (plateaus) in the workload trend, over momentary spikes.
We expect momentary spikes to be fairly common in workload data. But it's the plateaus that cause
request buffers to fill up, and hog server-side resources for extended periods of time thus
causing noticeable performance anomalies.

\subsection{Bottleneck Identification}