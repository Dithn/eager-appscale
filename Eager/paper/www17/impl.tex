\textcolor{blue}{This section shows that Roots is more than just a good idea.
Perhaps a figure showing the implementaion components and architecture would
help make that point stronger.}

We implemented a Roots prototype for the AppScale cloud
platform~\cite{6488671}. AppScale is an open source PaaS cloud, API compatible
with the popular Google App Engine (GAE) cloud platform~\cite{gae}.  Due to
the API compatibility, any application developed for GAE can be deployed and
executed on AppScale with no code changes. Since AppScale is open source
software, we were able to modify parts of its implementation to integrate the
Roots APM into it. AppScale also turned out to be a great test environment
since we could deploy it locally, and run a number of existing App Engine
applications on it.

We use ElasticSearch~\cite{elasticsearch} as the data storage component of our prototype. ElasticSearch is ideal 
for storing large volumes of structured and semi-structured data. It supports scalability and 
high availability via sharding and replication.
ElasticSearch continuously organizes and indexes data, making the information available 
for fast retrieval and efficient querying. Additionally it also provides
powerful data filtering and aggregation features, which greatly simplify the implementations of high-level
data analysis algorithms.

We configure AppScale's front-end server (based on Nginx) to tag all incoming application requests
with a unique identifier. This identifier is attached to the request as a custom HTTP header.
All data collecting agents in the cloud extract this identifier, and include it as an attribute
in all the events reported to ElasticSearch. This enables our prototype to aggregate events originating
from the same application.

We implement a number of data collecting agents in AppScale to gather runtime information
from all major components. These agents buffer data locally, and store them in ElasticSearch
in batches. For scraping server logs and storing the extracted entries in ElasticSearch,
we use the Logstash tool~\cite{logstash}. Logstash supports scraping a wide range of standard log formats (e.g. 
Apache HTTPD access logs), and other custom log formats can be supported via a simple configuration.
It also integrates naturally with ElasticSearch.
To capture the PaaS kernel invocation data, we augment AppScale's PaaS SDK implementation,
which is derived from the GAE PaaS SDK. More specifically we implement an agent that records
all PaaS SDK calls, and reports them to ElasticSearch asynchronously. 

Roots pods are implemented as standalone Java server processes. Threads are used to run benchmarkers,
anomaly detectors and handlers concurrently within each pod. Pods communicate with ElasticSearch via
REST calls, and many of the data analysis tasks such as filtering and aggregation are performed
in ElasticSearch itself. By doing so a lot of the heavy computations are offloaded to the 
ElasticSearch cluster, which is specifically designed for high-performance query processing
and analytics. Some of the more sophisticated statistical analysis tasks (e.g. change point detection, 
linear regression) are implemented in R language,
and the Java-based Roots pods integrate with R using the Rserve protocol~\cite{Urbanek03rserve--}.

\subsection{SLO-based Anomaly Detector}
We implement a performance SLO checker as the primary anomaly detector in Roots. This anomaly detector
allows application developers to specify simple performance SLOs for deployed applications. A
performance SLO consists of an upper bound on the application response time ($T$), and the probability ($p$)
that the application response time falls under the specified upper bound. Therefore, a general performance 
SLO can be stated as: ``application responds under $T$ milliseconds $p$\% of the time''.

When activated for a given application this anomaly detector starts a benchmarking process
that periodically measures the response time of the target application. The detector then periodically
analyzes the collected response time measurements to check if the application meets the specified performance
SLO. Whenever it detects that the application has failed to meet the SLO, it triggers an anomaly event. It would also
purge any past response time measurements (up to the anomaly) from the memory 
when this happens.

The SLO-based anomaly detector supports following configuration parameters:
\begin{itemize}
\item Performance SLO: Response time upper bound ($T$), and the probability ($p$).
\item Sampling rate: Rate at which the target application is benchmarked.
\item Analysis rate: Rate at which the anomaly detector checks whether the application has failed to meet the SLO.
\item Minimum samples: Minimum number of samples to collect before checking for SLO violations.
\item Window size: Length of the sliding window (in time) to consider when checking for SLO violations. This
acts as a limit on the number of samples to keep in memory.
\end{itemize}

\textcolor{blue}{Discuss this next paragraph as a performance tradeoff.  
Either you can purge each time an anomaly is detected and wait until you have
a statisticlly significant sample again (warm up period), OR you can redetect
the anomaly because you are analyzing the same data OR you can try to identify
which pieces of data in the past were responsible for an anomaly detection so
you can detect duplicates.  This last alternative is potentially
computationally expensive because it would mean that you would need to record
the specific entries associated with an anomaly in your DB each time and them
compare this set each time.  You chose warm up.  Say why.}
 
Since the detector purges past measurements when an anomaly is detected, it is not able to
check for further SLO violations until the minimum number of samples is collected. Therefore,
each anomaly is followed by a ``warm up'' period. With a sampling rate of 15 seconds, and a minimum
samples count of 100, the warm up period can last up to 25 minutes. The detector cannot find new
anomalies during this period. However, it prevents the same anomaly from being needlessly
reported multiple times.

\subsection{Path Distribution Analyzer}

\textcolor{blue}{Need to say that paths are generated via static analysis and
which tools you use.  The output of this analysis must be available to the
input of this anomaly detector.  You should also include this relationship in
your implementation architecture since it isn't obvious how Roots get call
path information or whether it is a comonent of Roots or a components of a
specific anomaly detector.}

Path distribution analyzer is another special anomaly detector we implement in Roots. This
anomaly detector periodically analyzes the PaaS kernel invocations made by the applications.
By aggregating the PaaS kernel invocations by application request identifiers, and then sorting them by
their sequence numbers, this anomaly detector is able to identify the sequence of
PaaS kernel invocations made by each application request. 
Each identified invocation sequence corresponds to a path of
execution through the application code (i.e. a path through the control flow graph of the application). 
Then the anomaly detector evaluates the number of requests
that invoked the same PaaS kernel invocation sequence. From that the anomaly detector
computes the distribution of different execution paths of an application.

A path distribution is comprised of the set of execution paths available in an application, along
with the proportion of requests that executed each path.
It is an indicator of the type of request workload handled by an application.
For example consider a data management application that has a read-only execution path, and a read-write 
execution path. If 90\% of the requests execute the read-only path, and the remaining 10\% of the requests
execute the read-write path, we may characterize the request workload as mostly read-only. 
Roots path distribution analyzer facilitates computing the path distribution for each application
with no static analysis or runtime instrumentation on the application code.

Roots path distribution analyzer periodically computes the path distribution for a given application.
If it detects that the latest path distribution is significantly different from the distributions seen in the 
past, it triggers an anomaly. This is done by computing the mean request proportion for each path
(over a sliding window of historical data),
and then comparing the latest request proportion values against the means. If the latest proportion
is off by more than $n$ standard deviations from its mean, the detector considers it to be an
anomaly. The sensitivity of the detector can be configured by changing the value of $n$, which
defaults to 2. 

This anomaly detector enables developers to know when the nature of their application request
workload changes. For example in the previous data management application, if suddenly 90\%
of the requests start executing the read-write path, the Roots path distribution analyzer will
detect the change as an anomaly. Similarly it is also able to detect when new paths of execution
are being invoked by requests (a form of novelty detection).

\subsection{Workload Change Analyzer}
Performance anomalies can arise due to bottlenecks in the cloud platform or changes in the application
workload.
When Roots detects a performance anomaly (e.g. an application failing to meet its performance SLO),
we need to be able to determine which of the above two causes may be behind it.
To check if the workload of an application has changed recently, Roots uses a workload change analyzer.
This is implemented as an anomaly handler, which gets executed every time an anomaly detector
notifies of a performance anomaly. Note that this is different from the path distribution analyzer,
which is implemented as an anomaly detector. While the path distribution analyzer looks for changes in the
\textit{type} of the workload, the workload change analyzer looks for changes
in the workload \textit{size}. \textcolor{blue}{Is it size or is it rate?}
In other words, it determines if the target application has received more requests than usual, which
may have caused a performance degradation.

Workload change analyzer uses change point detection algorithms to analyze the historical trend of 
the application workload. We use the ``number of requests
per unit time'' as the metric of workload size. This information can be obtained from the Roots
data storage as a time series. Our implementation of Roots supports a number of well known change point
detection algorithms (PELT~\cite{doi:10.1080/01621459.2012.737745}, binary segmentation 
and CL method~\cite{chen1993joint}), any of which can be used to detect level shifts in the
workload size. Algorithms like PELT favor long lasting shifts (plateaus) in the workload trend, over momentary spikes.
We expect momentary spikes to be fairly common in workload data. But it's the plateaus that cause
request buffers to fill up, and hog server-side resources for extended periods of time thus
causing noticeable performance anomalies.

\subsection{Bottleneck Identification}

\textcolor{blue}{Okay -- so thus far, the paper has been about Roots as a
system.  Now it seems to have shifted to be about analysis which is independent
of Roots.  If this is the intention, then the intro needs to say that there
are two sets of results here: a description of a working system for doing
performance diagnostics as a PaaS service and a set of algorithms that
implement diagnosis of performance failures.  It must also defend against the
criticism that the generality of Roots is not demonstrated (this is the only
diagnostic we present).  Another way to go is to present Roots and the
diagnostic together and then to discuss generality at the end, in a discussion
section, or as future work.  Also, you will probably need a figure that shows
the three analysis methods and how you combine them.} 

This is the primary root cause identification mechanism in our implementation of Roots. Like the workload
change analyzer, this is also implemented as an anomaly handler. Applications running in the cloud 
consist of user code executed on the application server, and remote 
service calls to various PaaS kernel services. AppScale cloud
consists of the same kernel services present in the Google App Engine public cloud (datastore, memcache,
urlfetch, blobstore, user management etc.).
We consider each PaaS kernel invocation, and the code running on the application server as 
separate \textit{components}. Each application request causes one or more components to
execute, and any one of the components can become a bottleneck to cause performance anomalies.  
The purpose of bottleneck identification is to find, out of all
the components executed by an application, the one component that is most likely to have caused 
application performance to deteriorate.

Suppose an application makes $n$ PaaS kernel invocations ($X_1, X_2, ... X_n$) for each request. 
For any given application request,
Roots captures the time spent on each kernel invocation ($T_{X_1}, T_{X_2}, ... T_{X_n}$), and the 
total response time ($T_{total}$) of the request. These time values are related by the formula
$T_{total} = T_{X_1} + T_{X_2} + ... + T_{X_n} + r$, where $r$ is all the time spent in the resident 
application server executing user code. $r$ is not
directly measured in Roots, but it can be computed from the above formula 
since Roots captures $T_{total}$ and all other $T_{Xn}$ values. In our previous
work we have shown that PaaS-hosted web applications spend most of their time invoking PaaS kernel services.
Therefore we may state that $r \ll T_{X_1} + T_{X_2} + ... + T_{X_n}$.

Roots bottleneck identification mechanism selects up to four components as possible candidates
for the bottleneck. These four candidates are then further evaluated by a weighted algorithm to
determine the actual bottleneck in the cloud platform. We first describe how Roots selects the
bottleneck candidates.

\subsubsection{Relative Importance of PaaS Kernel Invocations} 
The purpose of this metric is to find the component that is contributing mostly towards the variance in the total
response time. We start by selecting a window $W$ in time which includes a sufficient number of application requests,
and ending at the point when the performance anomaly was detected. Note that for each application request
in $W$, we can fetch the total response time ($T_{total}$), and the time spent on individual PaaS kernel
services ($T_{X_n}$) from the Roots data storage.

Then we take all the $T_{total}$ values
and the corresponding $T_{X_n}$ values in $W$, and fit them to the linear regression model
$T_{total} = T_{X_1} + T_{X_2} + ... + T_{X_n}$. Here we leave $r$ out deliberately, since it is typically small. To prevent
any bad data from skewing the model, we also filter out requests where the $r$ value is too high. This
is done by computing the mean ($\mu_r$) and standard deviation ($\sigma_r$) of $r$ over the selected window, and removing 
any requests where $r > \mu_r + 1.65\sigma_r$.

Once the regression model has been computed, we run a relative importance algorithm~\cite{JSSv017i01} to rank each of the
regressors (i.e. $T_{X_n}$ values) based on their contribution to the variance of $T_{total}$. 
We use the LMG method~\cite{lmg80} which is resistant to multicollinearity, and provides a break down of the $R^2$ value of
the regression according to how strongly each regressor is influencing the variance of the dependent variable.
The relative importance values of the regressors add up to the $R^2$ of the linear regression. We consider
$1 - R^2$ (the portion of variance in $T_{total}$ not explained by the PaaS kernel invocations) as the relative importance of $r$. 
The component associated with the highest ranked regressor is a strong candidate
for the bottleneck that we are looking for. Statistically, this is the component that causes the application
response time to vary the most.

\subsubsection{Historical Trend of Relative Importance}
This is a simple extension of the previous metric. We divide the time window $W$ into equal-sized segments,
and compute the relative importance metrics for regressors within each segment. We also compute the
relative importance of $r$ within each segment. This way we can
obtain a time series of relative importance values for each regressor and $r$. These time series
represent how the relative importance of each component has changed over time.

We subject each time series to change point analysis to detect if the relative importance of any particular
variable has increased recently. If such a variable can be found, then the component
associated with that variable is also a potential candidate for the bottleneck. 
The candidate selected by this method represents is
a component whose performance has been stable in the past, and has become variable recently. 

\subsubsection{High Quantiles}
Next we turn our attention to the individual distributions of the $T_{X_n}$ and $r$. 
Recall that for each PaaS kernel invocation
$X_k$, we have a distribution of $T_{X_k}$ values in the window $W$. Similarly we
can also extract a distribution of $r$ values from $W$. Out of all the available distributions
we wish to find the one whose quantile values are the largest.
Specifically, we compute a high
quantile (e.g. 0.99 quantile) for each distribution. The component, whose distribution 
contains the largest quantile value
is chosen as another potential candidate for the bottleneck. This component can be considered
as taking a long time to execute in general.

\subsubsection{Tail End Values}
Finally, we further analyze each $T_{X_n}$ distribution and $r$ distribution to detect which one has the largest tail end values.
We are particularly interested in the values that are larger than a certain high quantile, such as the 0.99 quantile.
For each tail end value $t$, we compute the metric $P^q_t$. This is the percentage difference between $t$ and the
$q$ quantile of the corresponding distribution. The component, whose distribution distribution has
the largest $P^q_t$ is chosen as another potential performance bottleneck.
This is the case where a component has rare outliers which are much larger than the rest of 
the values in its distribution (i.e. point anomalies).

\subsubsection{Selecting the Performance Bottleneck}
Above four mechanisms can choose up to four candidates for the performance bottleneck. The number of
chosen candidates can be less than four, since the same candidate can get selected from more than
one mechanism. Note that each of the above four mechanisms are designed to look for different
types of bottlenecks. The relative importance method looks for a component that has consistently
high variance. The historical trend of relative importance facilitates finding a component whose
performance has only recently become highly variable. The high quantile method is good at
finding a component which is slow in general. The last method finds a component with poor
tail latency. If more than one of these methods choose the same candidate for the bottleneck,
we can claim it to the actual bottleneck with high confidence.

Based on the above intuition, we design a simple weighting mechanism to find the actual
bottleneck from the selected candidates. Any component chosen by the relative importance
method gets 4 points. Components chosen by the other three methods get 3 points each. 
We sum up the points assigned to each component, and the component with the highest
score is declared to be the bottleneck. The relative importance method acts as a tie breaker
in case the four methods select four different candidates. The performance anomalies we
are interested in are typically caused by long lasting performance issues in the system 
(pattern anomalies). Since the relative importance method looks for components with
consistently high variance, it is somewhat better at finding components
with lasting performance issues.
