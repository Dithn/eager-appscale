We implemented a Roots prototype for the AppScale cloud platform. AppScale is an open
source PaaS cloud, API compatible with the popular Google App Engine (GAE) cloud platform.
Due to the API compatibility, any application developed for the GAE cloud can be deployed
and executed on AppScale with no code changes. Since AppScale is open source software, we
were able to modify parts of its implementation to integrate the Roots APM into it. AppScale also
turned out to be a great test environment since we could deploy it locally, and run a number of
existing App Engine applications on it.

We use ElasticSearch as the data storage component of our prototype. ElasticSearch is ideal 
for storing large volumes of structured and semi-structured data. It supports scalability and 
high availability via sharding and replication.
ElasticSearch continuously organizes and indexes data, making the information available 
for fast retrieval and efficient querying. Additionally it also provides
powerful data filtering and aggregation features, which greatly simplify the implementations of high-level
data analysis algorithms.

We configure AppScale's front-end server (based on Nginx) to tag all incoming application requests
with a unique identifier. This identifier is attached to the request as a custom HTTP header.
All data collecting agents in the cloud extract this identifier, and include it as an attribute
in all the events reported to ElasticSearch. This enables our prototype to aggregate events originating
from the same application.

We implement a number of data collecting agents in AppScale to gather runtime information
from all major components. These agents buffer data locally, and store them in ElasticSearch
in batches. For scraping server logs and storing the extracted entries in ElasticSearch,
we use the Logstash tool. Logstash supports scraping a wide range of standard log formats (e.g. 
Apache HTTPD access logs), and other custom log formats can be supported via a simple configuration.
It also integrates naturally with ElasticSearch.
To capture the PaaS kernel invocation data, we augment AppScale's PaaS SDK implementation,
which is derived from the GAE PaaS SDK. More specifically we implement an agent that records
all PaaS SDK calls, and reports them to ElasticSearch asynchronously. 

Roots pods are implemented as standalone Java server processes. Threads are used to run benchmarkers,
anomaly detectors and handlers concurrently within each pod. Pods communicate with ElasticSearch via
REST calls, and many of the data analysis tasks such as filtering and aggregation are performed
in ElasticSearch itself. By doing so a lot of the heavy computations are offloaded to the 
ElasticSearch cluster, which is specifically designed for high-performance query processing
and analytics. Some of the more sophisticated statistical analysis tasks (e.g. change point detection, 
linear regression) are implemented in R language,
and the Java-based Roots pods integrate with R using the Rserve protocol.

\subsection{SLO-based Anomaly Detector}
We implement a performance SLO checker as the primary anomaly detector in Roots. This anomaly detector
allows application developers to specify simple performance SLOs for deployed applications. A
performance SLO consists of an upper bound on the application response time ($T$), and the probability ($p$)
that the application response time falls under the specified upper bound. Therefore, a general performance 
SLO can be stated as: ``application responds under $T$ milliseconds $p$\% of the time''.

When activated for a given application this anomaly detector starts a benchmarking process
that periodically measures the response time of the target application. The detector then periodically
analyzes the collected response time measurements to check if the application meets the specified performance
SLO. Whenever it detects that the application has failed to meet the SLO, it triggers an anomaly event. It would also
purge any past response time measurements (up to the anomaly) from the memory 
when this happens.

The SLO-based anomaly detector supports following configuration parameters:
\begin{itemize}
\item Performance SLO: Response time upper bound ($T$), and the probability ($p$).
\item Sampling rate: Rate at which the target application is benchmarked.
\item Analysis rate: Rate at which the anomaly detector checks whether the application has failed to meet the SLO.
\item Minimum samples: Minimum number of samples to collect before checking for SLO violations.
\item Window size: Length of the sliding window (in time) to consider when checking for SLO violations. This
acts as a limit on the number of samples to keep in memory.
\end{itemize}

Since the detector purges past measurements when an anomaly is detected, it is not able to
check for further SLO violations until the minimum number of samples is collected. Therefore,
each anomaly is followed by a ``warm up'' period. With a sampling rate of 15 seconds, and a minimum
samples count of 100, the warm up period can last up to 25 minutes. The detector cannot find new
anomalies during this period. However, it prevents the same anomaly from being needlessly
reported multiple times.

\subsection{Path Distribution Analysis}
