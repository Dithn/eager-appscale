We implemented a Roots prototype for the AppScale cloud platform. AppScale is an open
source PaaS cloud, API compatible with the popular Google App Engine (GAE) cloud platform.
Due to the API compatibility, any application developed for GAE can be deployed
and executed on AppScale with no code changes. Since AppScale is open source software, we
were able to modify parts of its implementation to integrate the Roots APM into it. AppScale also
turned out to be a great test environment since we could deploy it locally, and run a number of
existing App Engine applications on it.

We use ElasticSearch as the data storage component of our prototype. ElasticSearch is ideal 
for storing large volumes of structured and semi-structured data. It supports scalability and 
high availability via sharding and replication.
ElasticSearch continuously organizes and indexes data, making the information available 
for fast retrieval and efficient querying. Additionally it also provides
powerful data filtering and aggregation features, which greatly simplify the implementations of high-level
data analysis algorithms.

We configure AppScale's front-end server (based on Nginx) to tag all incoming application requests
with a unique identifier. This identifier is attached to the request as a custom HTTP header.
All data collecting agents in the cloud extract this identifier, and include it as an attribute
in all the events reported to ElasticSearch. This enables our prototype to aggregate events originating
from the same application.

We implement a number of data collecting agents in AppScale to gather runtime information
from all major components. These agents buffer data locally, and store them in ElasticSearch
in batches. For scraping server logs and storing the extracted entries in ElasticSearch,
we use the Logstash tool. Logstash supports scraping a wide range of standard log formats (e.g. 
Apache HTTPD access logs), and other custom log formats can be supported via a simple configuration.
It also integrates naturally with ElasticSearch.
To capture the PaaS kernel invocation data, we augment AppScale's PaaS SDK implementation,
which is derived from the GAE PaaS SDK. More specifically we implement an agent that records
all PaaS SDK calls, and reports them to ElasticSearch asynchronously. 

Roots pods are implemented as standalone Java server processes. Threads are used to run benchmarkers,
anomaly detectors and handlers concurrently within each pod. Pods communicate with ElasticSearch via
REST calls, and many of the data analysis tasks such as filtering and aggregation are performed
in ElasticSearch itself. By doing so a lot of the heavy computations are offloaded to the 
ElasticSearch cluster, which is specifically designed for high-performance query processing
and analytics. Some of the more sophisticated statistical analysis tasks (e.g. change point detection, 
linear regression) are implemented in R language,
and the Java-based Roots pods integrate with R using the Rserve protocol.

\subsection{SLO-based Anomaly Detector}
We implement a performance SLO checker as the primary anomaly detector in Roots. This anomaly detector
allows application developers to specify simple performance SLOs for deployed applications. A
performance SLO consists of an upper bound on the application response time ($T$), and the probability ($p$)
that the application response time falls under the specified upper bound. Therefore, a general performance 
SLO can be stated as: ``application responds under $T$ milliseconds $p$\% of the time''.

When activated for a given application this anomaly detector starts a benchmarking process
that periodically measures the response time of the target application. The detector then periodically
analyzes the collected response time measurements to check if the application meets the specified performance
SLO. Whenever it detects that the application has failed to meet the SLO, it triggers an anomaly event. It would also
purge any past response time measurements (up to the anomaly) from the memory 
when this happens.

The SLO-based anomaly detector supports following configuration parameters:
\begin{itemize}
\item Performance SLO: Response time upper bound ($T$), and the probability ($p$).
\item Sampling rate: Rate at which the target application is benchmarked.
\item Analysis rate: Rate at which the anomaly detector checks whether the application has failed to meet the SLO.
\item Minimum samples: Minimum number of samples to collect before checking for SLO violations.
\item Window size: Length of the sliding window (in time) to consider when checking for SLO violations. This
acts as a limit on the number of samples to keep in memory.
\end{itemize}

Since the detector purges past measurements when an anomaly is detected, it is not able to
check for further SLO violations until the minimum number of samples is collected. Therefore,
each anomaly is followed by a ``warm up'' period. With a sampling rate of 15 seconds, and a minimum
samples count of 100, the warm up period can last up to 25 minutes. The detector cannot find new
anomalies during this period. However, it prevents the same anomaly from being needlessly
reported multiple times.

\subsection{Path Distribution Analyzer}
Path distribution analyzer is another special anomaly detector we implement in Roots. This
anomaly detector periodically analyzes the PaaS kernel invocations made by the applications.
By aggregating the PaaS kernel invocations by application request identifiers, and then sorting them by
their sequence numbers, this anomaly detector is able to identify the sequence of
PaaS kernel invocations made by each application request. 
Each identified invocation sequence corresponds to a path of
execution through the application code (i.e. a path through the control flow graph of the application). 
Then the anomaly detector evaluates the number of requests
that invoked the same PaaS kernel invocation sequence. From that the anomaly detector
computes the distribution of different execution paths of an application.

A path distribution is comprised of the set of execution paths available in an application, along
with the proportion of requests that executed each path.
It is an indicator of the type of request workload handled by an application.
For example consider a data management application that has a read-only execution path, and a read-write 
execution path. If 90\% of the requests execute the read-only path, and the remaining 10\% of the requests
execute the read-write path, we may characterize the request workload as mostly read-only. 
Roots path distribution analyzer facilitates computing the path distribution for each application
with no static analysis or runtime instrumentation on the application code.

Roots path distribution analyzer periodically computes the path distribution for a given application.
If it detects that the latest path distribution is significantly different from the distributions seen in the 
past, it triggers an anomaly. This is done by computing the mean request proportion for each path
(over a sliding window of historical data),
and then comparing the latest request proportion values against the means. If the latest proportion
is off by more than $n$ standard deviations from its mean, the detector considers it to be an
anomaly. The sensitivity of the detector can be configured by changing the value of $n$, which
defaults to 2. 

This anomaly detector enables developers to know when the nature of their application request
workload changes. For example in the previous data management application, if suddenly 90\%
of the requests start executing the read-write path, the Roots path distribution analyzer will
detect the change as an anomaly. Similarly it is also able to detect when new paths of execution
are being invoked by requests (a form of novelty detection).

\subsection{Workload Change Analyzer}
Performance anomalies can arise due to bottlenecks in the cloud platform or changes in the application
workload.
When Roots detects a performance anomaly (e.g. an application failing to meet its performance SLO),
we need to be able to determine which of the above two causes may be behind it.
To check if the workload of an application has changed recently, Roots uses a workload change analyzer.
This is implemented as an anomaly handler, which gets executed every time an anomaly detector
notifies of a performance anomaly. Note that this is different from the path distribution analyzer,
which is implemented as an anomaly detector. While the path distribution analyzer looks for changes in the
\textit{type} of the workload, the workload change analyzer looks for changes in the workload \textit{size}.
In other words, it determines if the target application has received more requests than usual, which
may have caused a performance degradation.

Workload change analyzer uses change point detection algorithms to analyze the historical trend of 
the application workload. We use the ``number of requests
per unit time'' as the metric of workload size. This information can be obtained from the Roots
data storage as a time series. Our implementation of Roots supports a number of well known change point
detection algorithms (PELT, binary segmentation and CL method), any of which can be used to detect level shifts in the
workload size. These algorithms favor long lasting shifts (plateaus) in the workload trend, over momentary spikes.
We expect momentary spikes to be fairly common in workload data. But it's the plateaus that cause
request buffers to fill up, and hog server-side resources for extended periods of time thus
causing noticeable performance anomalies.

\subsection{Bottleneck Identification}
This is the primary root cause identification mechanism in our implementation of Roots. Like the workload
change analyzer, this is also implemented
as an anomaly handler. When a performance anomaly is detected, this mechanism attempts to find the
most likely component in the PaaS kernel that may have caused the anomaly. AppScale PaaS kernel
consists of the same core services present in the Google App Engine public cloud (datastore, memcache,
urlfetch, blobstore, user management etc.). The purpose of bottleneck identification is to find, out of all
the PaaS kernel services used in an application, the one service that is most likely to have caused 
application performance to deteriorate. We use a fine grained approach so that when an application
invokes the same service multiple times, our implementation can pinpoint the exact service call that
contributed towards the performance anomaly. If none of the service calls cannot be associated with
the anomaly, we attribute the anomaly to the rest of the application code executed
directly in the application server.

Suppose an application makes $n$ PaaS kernel invocations ($X_1, X_2, ... X_n$). For each user request processed
by the application Roots captures the time spent on each kernel invocation ($T_{X1}, T_{X2}, ... T_{Xn}$), and the 
total response time ($T_{total}$) of the request. These time values are related by the formula
$T_{total} = T_{X1} + T_{X2} + ... + T_{Xn} + r$, where $r$ is all the time spent not invoking any PaaS kernel
services. $R$ is mainly the time spent in the resident application server executing user code, and it is not
directly measured in Roots. In our previous
work we have shown that PaaS-hosted web applications spend most of their time invoking PaaS kernel services.
Therefore we have $r \ll T_{X1} + T_{X2} + ... + T_{Xn}$.

Roots bottleneck identification mechanism uses above parameters to compute four metrics. These four metrics
are then further evaluated by a weighted algorithm to determine the bottleneck in the cloud platform. We
first describe the four metrics computed by Roots.

\subsubsection{Relative Importance of PaaS Kernel Invocations} 
The purpose of this metric is to find the service call that is contributing mostly towards the variance in the total
response time. We start by selecting a window $W$ in time which includes a sufficient number of application requests,
and ending at the point when the performance anomaly was detected. Note that for each application request
in $W$, we can find the total response time ($T_{total}$), and the time spent on individual PaaS kernel
services ($T_{Xn}$).

Then we take all the $T_{total}$ values
and the corresponding $T_{Xn}$ values in $W$, and fit them to the linear regression model
$T_{total} = T_{X1} + T_{X2} + ... + T_{Xn}$. Here we leave $r$ out deliberately, since it is typically small. To prevent
any bad data from skewing the model, we also filter out requests where the $r$ value is too high. This
is done by computing the mean ($\mu_r$) and standard deviation ($\sigma_r$) of $r$ over the selected window, and removing 
any requests where $r > \mu_r + 1.65\sigma_r$.

Once the regression model has been computed, we run a relative importance algorithm to rank each of the
regressors (i.e. $T_{Xn}$ values) based on their contribution to the variance of $T_{total}$. 
We use the LMG method which is resistant to multicolinearity, and provides a break down of the $R^2$ value of
the regression according to how strongly each regressor is influencing the variance of the dependent variable.
The kernel invocation associated with the highest ranked regressor (i.e. the one that contributes mostly 
towards the variance) is a very strong candidate
for the bottleneck that we are looking for. Statistically, this is the kernel invocation that causes the application
response time to vary the most.

\subsubsection{Historical Trend of Relative Importance}
This is a simple extension of the previous metric. We divide the time window $W$ into equal-sized continuous segments,
and compute the relative importance metrics for regressors within each segment. This way we can
obtain a time series of relative importance values for each regressor. These time series
represent how the relative importance of each variable has changed over time.

We subject each time series to change point analysis to detect if the relative importance of any particular
regressor has increased recently. If such a regressor can be found, then the PaaS kernel invocation
associated with that variable is also a potential candidate for the bottleneck. 