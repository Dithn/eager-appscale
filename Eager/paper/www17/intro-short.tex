Over the last decade cloud computing has become a popular approach for
deploying applications at
scale~\cite{Antonopoulos:2010:CCP:1855007,Pinheiro:2014:ACC:2618168.2618188}.
This widespread adoption of cloud computing, particularly for deploying web
applications, is facilitated by ever-deepening software abstractions.  While
abstractions elide much of 
the complexity necessary to enable scale, they also obscure
the runtime details, making the diagnosis of performance
problems challenging.  Therefore, the rapid expansion of cloud technologies
combined with their increasing opacity has intensified the need for new
techniques to monitor applications deployed in cloud
platforms~\cite{DaCunhaRodrigues:2016:MCC:2851613.2851619}. 

Application developers and cloud administrators wish to monitor
application performance, detect anomalies, and identify performance 
bottlenecks. To
obtain this level of operational insight in a cloud,
the cloud platform must support data gathering and analysis capabilities that
span the entire software stack.  However, most cloud technologies
available today do not provide adequate means to monitor applications or the cloud
services they depend on, in a way that facilitates identifying performance bottlenecks
within the cloud platform.

Application-level instrumentation packages are
plentiful~\cite{newrelic,datadog,dynatrace}, but because they perturb the
applications they can significantly increase the
effort and financial cost of application development and maintenance.  Moreover, cloud
administrators must trust the application developers to instrument their
applications correctly (e.g. logging all the relevant 
information to diagnose a rare performance fault).  Even when good application
instrumentation is available, however, because applications depend 
so heavily on
extant cloud services (e.g. database services, in-memory caching, etc.) that
are performance opaque,
it is often difficult, if not impossible, to diagnose
the ``root cause'' of a performance problem.

%Cloud
%administrators must therefore trust the application developers to implement
%necessary logging and instrumentation at the application level. This typically
%entails using third party, external monitoring
%software~\cite{newrelic,datadog,dynatrace}, which significantly increases the
%effort and financial cost of maintaining applications.  Developers must also
%ensure that their instrumentation is both correct, and does not degrade
%application performance.  Nevertheless, since the applications depend on
%extant cloud services (e.g. database services, in-memory caching, etc.) that
%are performance-opaque, it is often difficult, if not impossible to diagnose
%the ``root cause'' of a performance problem using extrinsic forms of
%monitoring.

%reporting application performance.
%
%only provide primitive
%monitoring features such as application-level logging. Hence, their support for performance
%anomaly detection and bottleneck identification is severely limited.

Further compounding the performance
diagnosis problem, today's cloud platforms are 
large and complex~\cite{DaCunhaRodrigues:2016:MCC:2851613.2851619,Ibidunmoye:2015:PAD:2808687.2791120}. 
They are
comprised of many layers, where each layer may consist of many interacting components.
Therefore when a performance anomaly manifests in a user application, it is
often challenging
to determine whether the anomaly's cause is in the platform software
and the exact software component that is responsible. 
Facilitating this type root cause analysis requires
both data collection at different layers of the cloud, and mechanisms for correlating 
the events recorded at different layers~\cite{7420511}. 
%
%Today's cloud platforms do not support such fine-grained
%data collection across the board. 
%
%The plethora of existing third party cloud monitoring solutions
%do not have visibility into the low-level activities of the cloud thereby rendering them incapable
%of performing systemwide root cause analysis.

%Moreover, performance monitoring for cloud applications needs to be highly customizable. Different
%applications have different monitoring requirements in terms of data gathering frequency (sampling rate), 
%length of the history to consider when performing statistical analysis (sample size), and the performance 
%SLOs (service level objectives~\cite{XXX}) that govern the application.
%%to maintain over time and check for
%%violations. 
%Cloud monitoring should be able to facilitate these diverse requirements on a
%per-application basis.
%Designing such customizable and extensible performance
%monitoring frameworks that are built into the cloud platforms is a novel and challenging undertaking.

In this paper, we present \textit{Roots} --  
a full-stack application platform 
monitor (APM) that is designed to be integrated
into a variety of cloud Platform-as-a-Service (PaaS) technologies. 
PaaS clouds, in particular, provide abstractions that hide most of the 
details concerning application
runtime~\cite{Soni:2014:CCB:2592737.2592741}. 
To do so, they offer a set of scalable, managed cloud services that
provide an assortment of functionality,
which developers invoke from the application implementations.
%to compose 
%into applications.
%The implementation and deployment details of these services are completely opaque to the developers. 
%However, the existing cloud monitoring techniques rely on being able to capture data from all components 
%of an application, all the way down to the virtual machines that host them. Such methods are
%therefore not viable in a PaaS cloud that hides runtime details beneath a layer of managed services.
To be able to correlate application activity with cloud service events,
Roots must be able to introspect the entire platform software stack. Therefore we
have chosen to design it to operate
%we design Roots 
as another managed service built into the PaaS cloud. 
%Therefore
%it operates at the same level as the other services offered by the cloud platform. This way Roots can collect data
%directly from the internal service implementations of the cloud platform, thus gaining full visibility into all the 
%inner workings of an application. 
As an added benefit, by implementing Roots as an intrinsic PaaS service,
%It also enables Roots to 
it can function fully automatically in the background, without
requiring instrumentation of application code. Instead,
Roots intercepts and records events as the
application code invokes various service implementations of the PaaS cloud,
and then correlates them with specific application requests.
Roots also records the latency of each application request, and
of each application call to an
internal cloud service implementation. It uses batch operations and asynchronous 
communication to record events with the goal of introducing minimal overhead.

%The proposed
%APM is not an external system that monitors a cloud platform from the outside (as most APM systems today). 
%Rather, it integrates with
%the PaaS cloud from within thereby extending and augmenting the existing components of the PaaS cloud
%to provide comprehensive full stack monitoring. 
%We believe that this design decision is a key differentiator over existing cloud 
%application monitoring systems because (i) it is
%able to take advantage of the scaling, efficiency, deployment, fault tolerance, security, 
%and control features that the underlying cloud offers, 
%(ii) while providing low overhead end-to-end monitoring of cloud applications.
%
%Previous work has outlined several key requirements that need to be taken into consideration when
%designing a cloud monitoring system~\cite{DaCunhaRodrigues:2016:MCC:2851613.2851619,Ibidunmoye:2015:PAD:2808687.2791120}. 
%We incorporate a number of these features into our design:
%\begin{description}
%\item[Scalability] Roots is lightweight, and does not cause any noticeable overhead in 
%application performance. It also puts strict upper bounds on the volume of data kept in memory. 
%The persistent data is accessed on demand, and can be removed after their usefulness has expired.
%\item[Multitenancy] Roots facilitates configuring monitoring policies at the granularity of individual applications.
%Users can employ different statistical analysis methods to process the monitoring data in ways that are 
%most suitable for their applications.
%\item[Complex application architecture] We design Roots to collect data from the entire cloud stack 
%(load balancers, app servers, built-in PaaS services etc.). Roots is able to correlate data gathered
%from different parts of the cloud platform, and perform systemwide bottleneck identification.
%\item[Dynamic resource management] Cloud platforms are dynamic in terms of their magnitude 
%and topology. By augmenting all the key components along the request processing path of the cloud platform,
%we make sure that Roots capture all the critical runtime data. When new processes/components
%spring to life in the cloud platform, they inherit the same augmentations, and start reporting to Roots automatically.
%\item[Autonomy] Roots is designed to detect performance anomalies online, without manual intervention.
%When Roots detects a problem, it attempts to automatically identify the root cause by analyzing
%available workload and service invocation data. Data collection begins automatically for new
%applications as they are deployed.
%\end{description}
%

%Roots collects most of the data it requires by direct integration with various internal components 
%of the cloud platform. In addition to high-level metrics like request throughput
%Roots monitors applications by directly integrating with internal PaaS components.
%In addition to high-level metrics such as request throughput
%and latency, 
 %a manner that does not substantively
%increase request latency.
%\textcolor{blue}{The remainder of this paragraph should
%go in Section~\ref{sec:arch}.
%In addition, Roots employs a collection of lightweight  benchmarking
%processes to collect performance data regarding user applications. Both
%the benchmarking processes, and the data analysis processes are executed 
%out of the request processing flow of the cloud platform. Such processes can be
%grouped together, and managed as a single entity called a
%``Roots Pod''. Pods keep minimum state
%information regarding the applications they monitor and analyze. This enables
%a single pod to monitor a large number of applications. Each pod is self-contained,
%and therefore scalability and high availability can be achieved by running multiple pods (sharding),
%and running multiple replicas of the same pod.}

When Roots detects a performance anomaly in application request latency, 
it attempts to
identify the root cause of the anomaly by analyzing the previous workload 
data of the application,
and the performance of the internal PaaS services on which the application depends.
It determines if the detected anomaly was caused by a change in the
application workload (e.g. a sudden spike in the number of client requests), or an internal
bottleneck in the cloud platform (e.g. a slow database query). To facilitate
this analysis we propose a ``bottleneck identification'' methodology
for PaaS clouds. 
Our approach uses a combination of quantile analysis, change point detection
and linear regression to perform this form of root cause analysis. 

We test the efficacy of our approach with a working prototype of 
Roots using the AppScale~\cite{6488671} open source PaaS. 
%We evaluate the feasibility and the 
%efficacy of Roots by conducting a series of empirical trials using our prototype. 
Our results indicate that using information gathered from the entire cloud
stack to parameterize the bottleneck identification algorithm, Roots
makes remarkably accurate diagnoses.
% nearly 100\% of the time. 
We also demonstrate that Roots does not add a significant performance overhead
to the applications, and that a single Roots pod (our encapsulation abstraction for
data analysis processes) can monitor tens of thousands
of applications simultaneously.

Thus we summarize the contributions made by this paper as follows.
\begin{itemize}
\item We describe the architecture of Roots as an intrinsic PaaS
service, which works automatically without requiring or depending upon
application instrumentation.
\item We describe a statistical methodology for determining when an
application is experiencing a performance anomaly, and identifying the 
workload change or the application component that is responsible for the anomaly.
\item We demonstrate the effectiveness of the approach using a working PaaS
prototype and real web applications.
\end{itemize}

%Rest of this paper is organized as follows.
%Section~\ref{sec:background} sets the stage for introducing Roots by describing the domain of 
%PaaS clouds and discussing performance monitoring fundamentals. Section~\ref{sec:arch} 
%details the high level architecture of Roots along with the motivation behind our design choices.
%Section~\ref{sec:impl} describes our implementation of Roots, with a strong emphasis on our
%solution to the bottleneck identification problem in PaaS clouds. Section~\ref{sec:results} presents our
%experimental results. Then we discuss some related work and conclude. 
