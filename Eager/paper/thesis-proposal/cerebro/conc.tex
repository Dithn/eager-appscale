%%Web services and service oriented architecture encourage developers to create new applications by
%%composing existing services via their web APIs. But such integration of services makes it difficult to
%%reason about the performance and other non-functional properties of 
%%composite applications. To
%%facilitate such reasoning, web APIs should be exported for use
%%by applications with strong performance 
%%guarantees (SLAs).%, so that application developers are aware of 
%what to expect from the web APIs they consume. 

%%To this end,
%We have presented Cerebro, a system that predicts response time 
%bounds for web APIs deployed in PaaS clouds. 
%We
%choose PaaS clouds as the target environment of our 
%research due to their rapidly growing popularity as a technology
%for hosting scalable web APIs, and their SDK-based, restricted 
%application development model, 
%which makes it easier to analyze PaaS applications statically.
Cerebro is a system that predicts response time 
bounds for web APIs deployed in PaaS clouds.  It does so via 
static analysis to extract the sequence of cloud SDK 
calls made by a given web API code combined with
historical performance measurements of cloud SDK calls. 
%to predict 
%the response time of the web
%API. 
%The historical performance data of cloud SDK 
%calls are gathered using a special monitoring agent
%executed in the PaaS cloud, that periodically benchmarks and records 
%cloud SDK performance, independent of deployed applications and web APIs.
Cerebro employs QBETS, a non-parametric time series analysis and 
forecasting method, to analyze
cloud SDK performance data, and predict bounds
on response time that can be used as statistical ``guarantees'' with
associated guarantee probabilities.
Cerebro is intended for use during development and 
deployment phases of a web API, and 
precludes the need for continuous performance testing of the API code. 
Further, it does not interfere with run-time operation (i.e. it requires
no application instrumentation) making it scalable.

We have implemented a prototype of Cerebro for Google App Engine public PaaS
and AppScale private PaaS
%We have subjected our
%prototype to a myriad of tests to study its efficacy in terms of
%correctness, tightness of predictions, and duration of prediction 
%validity.  
and evaluate it using a set of representative
and open source web applications developed by others.  
Our findings indicate that the prototype can determine response time levels
that correspond to specific target SLAs.  These predictions are also durable,
with average validity times varying between one day and several weeks.
We also find that API consumers do not have to renegotiate SLAs often, and the maximum
number of times an API consumer must renegotiate an SLA over a period of 112 days is six.
Overall, this work shows that automatic definition of response-time SLAs for web APIs is practically
viable in real world cloud settings, and API consumer timeframes.

%And we use Cerebro to
%predict the 95th percentile of the API operation response time. 
%We find that:
%\vspace{-0.05in}
%\begin{itemize}
%\vspace{-0.05in}
%\item Cerebro achieves the desired correctness goal of 95\% for all the applications in both cloud environments.
%\vspace{-0.05in}
%\item Cerebro generates tight predictions (i.e.
%the predictions are similar to measured values) for most web APIs.  Because
%some operations and PaaS systems exhibit more variability in cloud SDK response
%time, 
%Cerebro must be conservative in some cases, and produce predictions that are less tight
%to meet its correctness guarantees.  
%\vspace{-0.05in}
%\item Cerebro requires a ``warm up'' period of up to 200 minutes to produce trustworthy predictions. Since PaaS systems are designed to run continuously, this is not an issue in practice. 
%\vspace{-0.05in}
%\item We can use a simple yet administratively useful model to identify when an 
%SLA becomes invalid to compute
%prediction validity durations for Cerebro.  The average duration of a valid
%Cerebro prediction is between 24 and 72 hours,
%and 95\% of the time this duration is at least 
%1.41 hours for App Engine and 1.95 hours for AppScale.
%\vspace{-0.05in}
%\end{itemize}
%\vspace{-0.05in}

\ignore{
In the current design, Cerebro's cloud SDK monitoring agent only monitors 
a predefined set of cloud SDK operations. In our future work we wish 
to explore the possibility of making this component more dynamic,
so that it automatically learns what operations to benchmark from the web APIs 
deployed in the cloud. This also includes learning the size and the form of the datasets
that cloud SDK invocations operate on, so that Cerebro can acquire more realistic
benchmarking data. Implementing these features might require non-invasively
instrumenting the deployed cloud applications. We also plan to investigate further how to better
handle data-dependent loops (iterative datastore reads) for different workloads. We are interested
in exploring the ways in which we can handle API codes with unpredictable execution patterns (e.g.
loops based on a random number), even though such cases are quite rare in the applications we
have looked at so far.
Further, we plan
to integrate Cerebro with EAGER, our API governance system 
and policy engine for PaaS clouds, so 
that PaaS administrators can enforce SLA-related policies on web APIs at deployment-time.
%Such a system will make it possible to prevent any API that 
%does not adhere to the organizational performance
%standards from being deployed in the production cloud environment.
}
