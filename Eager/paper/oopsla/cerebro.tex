Given the restricted application domain of PaaS-hosted web APIs, 
we believe that it is possible
to design a system that predicts response time SLAs for them using only
static information from the web API code itself.  To enable this, we design Cerebro
with three primary components:
\begin{itemize}
\item A static analysis tool that extracts sequences of cloud SDK operations for 
each path through a method (web API operation),
\item A monitoring agent that runs in the target PaaS and efficiently monitors 
the performance of the underlying cloud SDK operations, and
\item A prediction mechanism that uses the outputs of these two components to accurately predict an upper bound on the execution time of the web API.
\end{itemize}
In our prototype, we invoke Cerebro each time a web API is deployed or when requested
by a developer directly. Cerebro returns the worst case upper bound for a 
given SLA target probability for the web API.  
Cerebro allows PaaS administrators to define API governance policies that 
allow an application to be deployed only when its web APIs will meet a 
pre-determined (or pre-negotiated) SLA target and to be notified 
by the platform when such SLAs require renegotiation.

\subsection{Static Analysis}
 This component analyzes the source code of the web API
(or some intermediate representation of it) and extracts a sequence of cloud API operations.
We implement our analysis for Java bytecode programs
using the Soot framework~\cite{Vallee-Rai:2010:SJB:1925805.1925818}.
Currently, our prototype analyzes the considers the 
following as API methods.
\begin{itemize}
\item classes that extend the \textit{javax.servlet.HttpServlet} class (i.e. Java servlet implementations)
\item classes that contain JAXRS \textit{@Path} annotations, and
\item any other classes explicitly specified by the developer in a special configuration file (none in our current prototype).
\end{itemize}

Cerebro performs a simple construction and interprocedural static analysis 
of a method's control flow graph 
(CFG)~\cite{Allen:1970:CFA:800028.808479,Aho:1986:CPT:6448,Morgan:1998:BOC:288765,Muchnick:1998:ACD:286076} for each API method.
The algorithm extracts all cloud SDK operations along
each path through the method.  Cerebro analyzes calls to other web API functions
that the method calls, recursively.  Cerebro caches cloud SDK details for each function 
once analyzed so that it can be reused efficiently for other call sites to the same
function. Cerebro does not analyze third-party library calls, if any.  Cerebro encodes
each cloud SDK call sequence for each path in a lookup table. We identify
cloud SDK calls by their Java package name (e.g. \texttt{com.google.appengine.apis}).

To handle loops, we first extract them from the CFG and 
annotate all cloud SDK invocations that occur within them.
We annotate each with an estimate on the number of times
the loop is likely to execute in the worst case. 
We estimate loop bounds using a loop bound prediction algorithm 
based on abstract interpretation~\cite{bygde2010static}. 


As shown in the previous section, loops in these programs 
are rare and, when they do occur, they are
used to iterate over a particular data sets returned from a database.
For such data dependent loops, we estimate the bounds if specified 
in the cloud SDK call (e.g. the maximum number of 
entities to return~\cite{gae-fetch-options}).
If our analysis is unable to estimate the bounds for these loops, Cerebro prompts
the developer for an estimate of the likely data set size and/or loop bounds.

\subsection{PaaS Monitoring Agent}
Cerebro monitors and records the response time of individual
cloud SDK operations within a running PaaS system.  Such support can be 
implemented as a PaaS-native feature or as
a PaaS application (web API); we use the latter in our prototype.
The monitoring agent runs in the background with, but is separate from, 
other PaaS-hosted web APIs.
The agent invokes cloud SDK operations periodically on synthetic datasets and 
records timestamped response times in the PaaS datastore for each cloud SDK
operation.
Finally, the agent periodically reclaims
to eliminate unnecessary storage. The Cerebro monitoring and reclamation 
rates are configurable and monitoring benchmarks can be added and customized
easily to capture common PaaS-hosted web API patterns.

In our prototype, the agent monitors the Datastore and Memcache services
every 60 seconds.  In addition, it 
benchmarks loop iteration over datastore entities to capture
the performance of iterative datastore reads for datastore result set sizes 
of 10, 100, and 1000.

\subsection{Making SLA Predictions}

To make SLA predictions, Cerebro uses 
Queue Bounds Estimation from Time Series (QBETS)~\cite{Nurmi:2007:QQB:1791551.1791556},
a non-parametric time series analysis method that we developed in prior work.
We originally designed QBETS for
predicting the scheduling delays for the batch queue systems 
used in high performance computing environments. 
We adapt it herein for use ``as-a-service'' in PaaS systems 
to predict the execution time of web APIs.

A QBETS analysis requires three inputs:
\begin{enumerate}
\item A time series of data generated by a continuous experiment,
\item The percentile for which an upper bound should be predicted ($p \in [1..99]$)
\item The upper confidence level of the prediction ($c \in (0,1)$)
\end{enumerate}

QBETS uses this information to predict an upper bound for 
the $p$-th percentile of the time series.
The predicted value has a probability of $0.01p$ of 
being greater than or equal to the next data point that
will be added to the time series by the continuous experiment. 
The upper confidence level $c$ serves as a conservative
bound on the predictions. That is, predictions made with an upper confidence 
level of $c$ will overestimate
the true percentile with a probability of $1-c$. This confidence guarantee 
is necessary because QBETS does not determine the 
percentiles of the time series precisely, but only estimates them. 
If not provided as part of Cerebro invocation, our prototype 
defaults to $p=95$ and $c=0.05$. 

For example, assume a continuous experiment 
in which we periodically measure the
response time of a web API. Doing so results in a time series of 
response time data. Suppose at time $t$,
we run QBETS on the time series data collected so far 
with $p=95$ and $c=0.01$. The prediction returned
by QBETS has a 95\% chance of being greater than or equal 
to the next response time value measured
by our experiment after time $t$. Since $c=0.01$, the predicted value has a 99\% chance of
overestimating the true 95th percentile of the time series.

QBETS requires a sufficiently large number of data points
in the input time series before it can make an accurate prediction. 
Specifically, we have proved that QBETS must to see 
at least $log(c)/log(0.01p)$ data points
before it can start making reliable predictions. 
This means, if we are interested in predicting the 95th percentile
of the API execution time, with an upper confidence of 0.01, 
Cerebro must feed QBETS with a time series that
contains at least 90 data points. We use this information to control 
reclamation of monitoring data.

\subsection{Example of the Cerebro Process}
Upon invoking Cerebro with a web API, Cerebro
performs its static analysis on each of API operation 
and produces a list of annotated cloud SDK invocation sequences
for each path in each.  Cerebro prunes each list to eliminate duplicates.
Duplicates occur when a web API code has
multiple program paths with the same sequence of cloud SDK invocations.
Then for each pruned list, Cerebro
performs the following operations:
\begin{enumerate}
\item Retrieve benchmarking data from the monitoring agent 
for all operations in the sequence. The agent returns
an ordered time series data (one time series per cloud SDK operation).
\item Align retrieved time series across operations and aggregate
to form a single time series for the sequence of cloud SDK operations.
\item Pass the aggregate time series to QBETS along with the 
desired $p$ and $c$ values to predict an upper bound. 
\end{enumerate}

For example, suppose our static analysis results in the
cloud SDK invocation sequence $<op_{1},op_{2},op_{3}>$. 
Lets assume the monitoring agent has the following
time series data collected for the three operations in the sequence.
\begin{itemize}
\item $op_{1}$: $[t_{0}: 5, t_{1}: 4, t_{2}: 6, ...., t_{n}: 5]$
\item $op_{1}$: $[t_{0}: 22, t_{1}: 20, t_{2}: 21, ...., t_{n}: 21]$
\item $op_{1}$: $[t_{0}: 7, t_{1}: 7, t_{2}: 8, ...., t_{n}: 7]$
\end{itemize}

Here $t_{m}$ are timestamp values the monitoring agent has assigned to 
each data point. Cerebro aligns the three
time series according to timestamp 
and aggregates the results
to obtain the following time series:
$[t_{0}: 34, t_{1}: 31, t_{2}: 35, ...., t_{n}: 33]$

Cerebro passes the aggregate time series to QBETS for analysis, and
treats the QBETS prediction as an SLA for the web API operation.
If the QBETS predicted value is $Q$ milliseconds, 
we can form the SLA as ``the web API responds 
under $Q$ milliseconds, $p$ percent of the time''. 
When the code has multiple operations and operations have 
paths of execution
Cerebro estimates multiple SLAs for the API. In
such cases we treat the SLA with highest predicted value 
as the final SLA (i.e. the worst case SLA).

When the static analysis produces cloud SDK invocation 
sequences with information about loops, Cerebro performs an additional step.
If some operation has been tagged as being inside a loop, where the loop
bounds have been estimated, the time series data corresponding to that 
operation should be multiplied 
by the loop bound estimate, before aggregating. In cases where the operation 
is inside a data dependent loop, we request the time series data from 
the monitoring agent for its iterative datastore read benchmark 
for a number of entities that is equal to or larger than the annotation. 

