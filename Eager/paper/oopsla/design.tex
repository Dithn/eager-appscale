In this section we provide details of Cerebro's design. We explain its major components, their interactions and how the 
system works as a whole. Recall that the overarching theme of our solution is to combine program
static analysis data with cloud platform monitoring data to formulate an accurate performance model for the 
web APIs developed for a given cloud platform. Therefore, at very least Cerebro requires the following two components:

\begin{itemize}
\item A program static analysis component that can extract the sequence of cloud SDK operations invoked by a web API
code.
\item A monitoring agent that runs in the target cloud platform to monitor the performance of its individual cloud SDK
operations.
\end{itemize}
 
 \subsection{Program Static Analysis Component}
 The static analysis component analyzes the source code (or some intermediate representation of it), to identify
 the cloud SDK operations invoked by a given web API code. Since a web API code can only make a finite
 number of cloud SDK invocations, and since PaaS clouds only support a finite number of cloud
 SDK operations, it is possible to design an algorithm that
 extracts all cloud SDK invocations from any given web API code. 

The basic idea behind Cerebro's static analysis is to construct and walk the
 control flow graph (CFG) of the given code. Whenever the algorithm encounters a node that represents a function call,
 it checks whether the function call corresponds to a cloud SDK operation. If so the algorithm
adds it to the output of the analysis. At the end of the analysis, the algorithm outputs a list whose members are
sequences of cloud SDK operations. Each sequence of operations corresponds to a different
path of execution through the code.

Listing 1 shows the outline of the basic program analysis algorithm used by Cerebro. 
Constructing the CFG from a given program
is a common problem with many efficient solutions. Graph traversal is performed in the depth-first 
fashion to ensure that the algorithm walks each path of execution to the end of the program, before picking up on
a new path. The algorithm marks the nodes in the CFG as it visits them to ensure that the graph traversal does not get stuck in loops. 

If the algorithm detects any function call nodes that correspond to functions in the user's code, the algorithm
starts analyzing the code of the target functions recursively. This effectively turns our analysis into an inter procedural
CFG analysis. Results obtained from analyzing user-implemented functions can be cached to avoid
having to analyze the same function more than once. Note that the algorithm simply appends the results of
analyzing user-implemented functions to the current sequence of cloud SDK invocations. This ensures that the final output of the
analysis contains cloud SDK invocation sequences spanning across all the relevant functions in the code. 
In other words, the operation sequences in the final output may cross function boundaries.
Since any web API considered for analysis must have a finite amount of code, as long as
we mark the visited nodes in the CFG, and keep track of the functions that we have already analyzed, this
analysis is guaranteed to terminate with results.

In Cerebro we do not recursively analyze any of the cloud
SDK operations or any third party library calls. 
Statically analyzing the internals of cloud SDK invocations is
unnecessary since we intend to build a performance model for web APIs where
the smallest unit of operation is a cloud SDK call (we simply do not need to dig any deeper). As for third 
party libraries, we assume they do not contain any cloud SDK invocations -- a reasonable assumption 
regarding most third party libraries that are generic and intended to run on a variety of target runtimes. 
 
Cerebro's static analysis is mostly comprised of well-understood, general purpose program analysis techniques.
 Perhaps the only application-specific element here is the method used to check function call nodes for cloud SDK invocations.
 In the worst case this check can be implemented as a table lookup. That is, we maintain a table of all possible cloud
 SDK operations, and check whether each function call
 corresponds to an entry in the table. In real world implementations there are other more efficient methods that
 can be employed to solve this problem. For instance some PaaS clouds use a special naming convention to label
 their cloud SDK operations. 
 In such a scenario we can simply check whether the function calls in CFG nodes match the
 desired pattern. 
 
 \subsection{Handling Loops}
The algorithm in listing 1 is only meant as an outline for a minimalistic program analysis that can be performed
within Cerebro. If needed more sophisticated analyses can be incorporated into this algorithm to extract more
useful information regarding the cloud SDK invocations. For example, we can perform a loop extraction on the
CFG to identify the loops in the code, and tag the cloud SDK invocations in the output to indicate which of them
are executed inside loops. In such an implementation, the output of the static analysis will not be a simple
list of cloud SDK invocation sequences. Rather, it will be a list of \textit{annotated} cloud SDK invocation sequences.

When there are cloud SDK invocations present inside loops, it would be useful to know how many times those loops
are going to execute. Clearly, any performance model based on cloud SDK calls made by a web API code,
needs to account for the repeated execution of cloud SDK calls inside loops. If the 
loops in question are data-independent (i.e. the iteration count does not depend on the size of the data processed 
by the code), we can use an static loop bound estimator to determine how many times the loops are going to iterate.
This can be done for each loop extracted during our CFG analysis, and the estimated loop bounds can be added to the
analysis output as annotations.

However, as indicated by our survey results, loops are fairly uncommon in web API codes developed for PaaS clouds. When
they are present, they usually iterate over some dataset (i.e. data-dependent). Such loops cannot
be accurately predicted using static analysis methods. But survey results also show us that
most of the time cloud web API codes iterate over datasets loaded from the underlying datastore of the PaaS.
We can use this as a heuristic to bound the iteration count of data-dependent loops. Typically, application developers
have an approximate idea of how large their database will be once their code is in production. When we encounter a data-dependent
loop in the CFG that we cannot accurately predict using a static loop bound estimator, we can simply
prompt the developer to enter a reasonable maximum size for the underlying database (i.e. the maximum number of
records/entities that may be returned from the datastore). We can
use these developer-specified values as upper bounds for the loop iteration count. 
This information can also be added to the final outcome of the static analysis as annotations.