Web services and service oriented architectures have
revolutionized the way developers implement software.
Using the web service model, developers ``mash up'' content and functionality 
from remote, web-accessible services exposed via application programming interfaces (APIs)
to create new web APIs, applications, and services.  By leveraging and reusing the 
engineering, testing, and software maintenance provided by others, programmer
productivity is significantly improved.

As a result, web APIs and their service implementations are rapidly 
proliferating.  At the time of this writing, 
the ProgrammableWeb~\cite{pweb}, a popular web API index, lists over $13,000$
publicly available web APIs.
These APIs increasingly employ the REST (Representational State Transfer) 
architectural style~\cite{Fielding:2000:ASD:932295}, and target both
commercial (e.g. advertising, shopping, travel, etc.) and non-commercial
(e.g. IEEE~\cite{ieeeapis}, UC Berkeley~\cite{ucbapis}, and the US White
House~\cite{whitehouseapis}) application domains.

Despite its many benefits, reusing running code that is implemented
and maintained by others, also has its costs.  
In particular, new applications and web APIs become dependent on the 
remote APIs they compose.  These dependencies
impact correctness, performance, and availability of the encapsulating 
application -- for which the developer becomes accountable.  
Compounding the situation, remote APIs can and do change over time, many times
unbeknownced to the developers of web APIs they comprise.
Unfortunately, there is a dearth of tools that help developers reason about these 
dependencies throughout an application's 
lifecycle (i.e. development, deployment, and runtime).  Without such tools, 
programmers must resort to extensive, continuous, and costly, testing and profiling 
for all of the remote APIs that their web APIs employ.

To address this need, we investigate Cerebro, a new approach that
\textit{statically} predicts the execution performance of web APIs that 
integrate remote APIs for their functionality.
To enable this, we employ the notion of service-level agreement (SLA) from 
public cloud computing systems such as Amazon Web Services (AWS)~\cite{amazon-aws-web} and 
Google App Engine~\cite{gae}.  SLAs are policies that specify
what a user can expect from a service in terms of uptime or error rate
before they receive a financial credit for
use of the service~\cite{aws-ec2-sla,aws-s3-sla,aws-rds-sla,gae-sla,gcs-sla}.
For example, AWS makes its Elastic Compute Cloud (EC2) and Elastic Block Store (EBS) 
available with a monthly uptime percentage 
of at least 99.95\%; Google makes Big Query and and App Engine services (e.g. the 
Bigtable-based Datastore) available with monthly uptime percentage of at least 
99.95\% where downtime is determined by a 5\% and 10\% error rate, respectively.
Cerebro extends this notion of SLA to the execution time (response time) 
of web APIs that are deployed via (i.e. hosted by)
cloud platform-as-a-service (PaaS) technologies (e.g. App Engine, 
AppScale~\cite{6488671}, Azure~\cite{azure-web}, etc.).
For example, an API operation responds in 100ms at least 95\% of the time.

We target cloud platforms (public and private) as our web API and Cerebro 
runtime system given their popularity and wide spread use~\cite{xxx,yyy,zzz}.
PaaS clouds provide developers with a collection scalable services exported 
as remote APIs (i.e. via cloud software development kits (cloud SDKs)), that simplify
development and that automate deployment of new applications.  
These services are fully managed 
and provided via availability SLAs by the platform. For example, the services 
of App Engine and AppScale (an open source, private version of App Engine)
include distributed NoSQL datastores, SQL databases, task management, 
and data caching, among others. Currently there are over four million active 
App Engine applications, including those from BestBuy, Snapchat, 
and Sony Music. 

Cerebro predicts execution time SLAs of a web API by combining static analysis
of the program with dynamic performance sampling of the cloud services exported
by the platform (i.e. the remote APIs made available via a cloud SDK)
as depicted in Figure~\ref{fig:overview}.
Cerebro uses static analysis to identify the cloud SDK operations
that dominate the execution time of web APIs. It then uses historical time series data 
of the performance of these services to estimate an upper bound on the execution 
time of the web API that integrates them.  In addition, we associate Cerebro 
predictions with a configurable level of success probability and conservativeness. 
Success probability is the fraction of API invocations whose execution 
times are less than or equal to the
predicted upper bound.
Conservativeness is the degree to which 
Cerebro is overestimating the upper bound on execution time, e.g. due to 
high variation in the performance of platform services.
The combination of these two metrics 
enable API developers to use Cerebro to predict performance SLAs for their
web APIs.

Cerebro is fully automated and requires no developer intervention.
It also precludes the need for deployment, load testing, and
instrumentation of the web APIs being analyzed. Our intention is for developers or platform 
administrators to invoke Cerebro at or just prior to web API deployment to the platform.
Because platform behavior and service implementations (remote APIs) can change
over time, Cerebro's predictions have a lifetime.  As part of this paper, we investigate
the effective lifetime of these predictions.  When platform performance changes over time
Cerebro can be reinvoked for any of the deployed web APIs to establish new SLAs or to
enforce SLA violation prevention through web API termination.

We have implemented Cerebro for both the App Engine public PaaS and 
the AppScale private PaaS. We evaluate the accuracy of Cerebro and the tightness
of the bounds it predicts (i.e. the difference between the prediction and 
the actual API execution times) using open source App Engine applications (web APIs).  
We also investigate the duration over which 
these predictions hold.  We find that Cerebro is able to achieve its SLAs with 
very high accuracy and that the predictions hold from 1.4 to over 24 hours.  
We also find that the high variability of the public PaaS due to multitenency
and massive scale requires that Cerebro be more conservative in its predictions
(i.e. they are less tight than those for the AppScale private, single tenent cloud), 
to achieve the same level of accuracy.

In the sections that follow, we first overview our approach and our assumptions 
in Section~\ref{sec:approach}.   We then
present the results of a performance analysis that we have performed which provides
insight into the performance characteristics of PaaS 
applications (Section~\ref{sec:survey}).  
In Section~\ref{sec:design}
and Section~\ref{sec:prototype_impl}, we explain the architecture and 
prototype of Cerebro, respectively.  We then 
present our empirical evaluation of the Cerebro prototype in 
Section~\ref{sec:results}.
Finally,  we discuss related work (Section~\ref{sec:related_work}) and 
conclude (Section~\ref{sec:conclusions}).
