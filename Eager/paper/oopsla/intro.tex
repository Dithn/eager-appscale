Web services, service oriented architectures, and cloud platforms have
revolutionized the way developers engineer and deploy software.
Using the web service model, developers create new applications
by ``mashing up'' content and functionality 
from existing services 
exposed via web-accessible application programming interfaces (web APIs).
This approach both expedites and simplifies implementation since 
developers can leverage the 
software engineering and maintenance efforts of others.
Moreover, platform-as-a-service (PaaS) clouds
for hosting web applications have emerged as a key
technology for managing applications at scale in both public (managed) and 
private settings.  

Consequently, web APIs are rapidly 
proliferating.  At the time of this writing, 
ProgrammableWeb~\cite{pweb} indexes over $13,000$
publicly available web APIs.
These APIs increasingly employ the REST (Representational State Transfer) 
architectural style~\cite{Fielding:2000:ASD:932295}, and target both
commercial (e.g. advertising, shopping, travel, etc.) and non-commercial
(e.g. IEEE~\cite{ieeeapis}, UC Berkeley~\cite{ucbapis}, US White
House~\cite{whitehouseapis}) application domains.

Despite the many benefits, reusing existing services also has its costs.  
In particular, new applications become dependent on the 
services they compose.  These dependencies
impact correctness, performance, and availability of the composite 
applications -- for which the ``top level'' developer is often held accountable.  
Compounding the situation, the underlying services can and do change over time
while their APIs remain stable,
unbeknownst to the developers that programmatically access them.
Unfortunately, there is a dearth of tools that help developers reason about these 
dependencies throughout an application's 
lifecycle (i.e. development, deployment, and runtime).  Without such tools, 
programmers must resort to extensive, continuous, and costly, testing and profiling 
to understand the performance impact on their applications
that results from the increasingly complex collection of
services that they depend on.

To address this issue, we present Cerebro, a new approach that
predicts bounds on 
the response time performance of web
APIs exported by applications that are hosted in a PaaS cloud.
The goal of Cerebro is to allow 
a PaaS administrator to determine what response time service-level agreement (SLA) can be
fulfilled by each web API operation exported by the applications
hosted in the PaaS.  

SLAs typically specify a minimum service level, and a
probability (usually large) that the minimum service level will be
maintained. Cerebro uses a combination of static analysis of the hosted web APIs, 
and runtime monitoring of the PaaS cloud (\textit{not} the web APIs themselves) 
to determine what minimum response time guarantee can be made 
with a target probability specified by a PaaS administrator. These calculated
SLAs enable developers to
reason about the performance of the applications that consume the cloud-hosted
web APIs.

Currently, cloud computing systems such as Amazon Web Services
(AWS)~\cite{amazon-aws-web} and
Google App Engine (GAE)~\cite{gae} offer reliability SLAs specifying the fraction of
availability over a fixed time period for their services
that users can expect, when they contract to use a service. However, they do not provide SLAs
guaranteeing minimum levels of performance.
In contrast, Cerebro predictions make it possible to determine response time SLAs with
probabilities specified by the cloud provider in a way that is scalable.

%of 
%integrate services as part of their functionality.
%To enable this, we employ the notion of service-level agreement (SLA) from 
%public cloud computing systems such as Amazon Web Services (AWS)~\cite{amazon-aws-web} and 
%Google App Engine~\cite{gae}.  SLAs are policies that specify
%what a developer can expect from a service in terms of availability or error rate,
%typically before they receive a financial credit for paid
%use of the service~\cite{aws-ec2-sla,aws-s3-sla,aws-rds-sla,gae-sla,gcs-sla}.
%For example, AWS makes its Elastic Compute Cloud (EC2) and Elastic Block Store (EBS) 
%available with a monthly uptime percentage 
%of at least 99.95\%; Google makes Big Query and and App Engine services (e.g. the 
%Bigtable-based datastore) available with monthly uptime percentage of at least 
%99.95\% where downtime is determined by a 5\% and 10\% error rate, respectively.
%Cerebro extends this notion of SLA to the execution time (response time) 
%of web APIs that are deployed via (i.e. hosted by) PaaS
%technologies (e.g. App Engine, 
%AppScale~\cite{6488671}, Azure~\cite{azure-web}, etc.).
%For example, a Cerebro SLA says that an API operation will respond to a HTTP request
%within 100ms at least 95\% of the time.

Cerebro is designed to improve the use of both public and private PaaS clouds 
which have emerged as popular application
hosting venues~\cite{paas-growth}.
For example, there are over four million active GAE
applications that 
can execute on Google's public cloud or over AppScale, an open source, private 
cloud version of App Engine.
A PaaS cloud provides developers 
with a collection of commonly used, scalable services,
that the platform exports via APIs defined within a software 
development kit (cloud SDK). These services are fully managed and covered under 
the availability SLAs of the cloud platform. For example, services 
in GAE and AppScale cloud SDK
include a distributed NoSQL datastore, task management, 
and data caching, among others. 

Cerebro generates response time SLAs for API calls exported by a web
application
developed using the services available within the PaaS.  For brevity, in this work
we will use the
term \textit{web API} to refer to a web-accessible API exported by an
application hosted on a PaaS platform. Further, 
we will use the term \textit{cloud
SDK} to refer to the APIs that are maintained as part of the PaaS and
available to all hosted applications. This enables us to
differentiate the internal APIs of the PaaS from the 
APIs exported by the deployed applications.   
For example, an application hosted in Google App Engine might export one or
more web APIs to its users while leveraging the internal cloud SDK for the
Google datastore that is available as part of the Google App Engine PaaS.

% that integrates such services 
%by combining static analysis
%of the program with dynamic performance sampling of the platform services.
Cerebro uses static analysis to identify the cloud SDK invocations
that dominate the response time of web APIs.  Independently,
Cerebro also maintains a running history of cloud SDK response time 
performance.  It uses
QBETS~\cite{Nurmi:2007:QQB:1791551.1791556} -- a forecasting methodology
we have developed in prior work for predicting bounds on ``ill behaved''   
univariate time series -- to predict response time bounds on each cloud SDK
invocation made by the application.  It combines these predictions dynamically
for each static program path through a web API operation,
and returns the ``worst-case''
upper bound on the time necessary to 
complete the operation.

%Cerebro also integrates a nonparametric
%prediction tool called QBETS~\cite{Nurmi:2007:QQB:1791551.1791556} that 
%we have developed in prior work for a completely different 
%purpose: batch queue prediction in high-performance computing 
%systems. Cerebro ``service-izes'' QBETS for use in a cloud platform and
%interacts with it
%dynamically to estimate an upper bound on the execution time of each service operation.
%Cerebro then combines this estimate with its static analysis to forecast an execution SLA 
%for the web API.

Because service implementations and platform behavior under load change over time,
Cerebro's predictions necessarily have a lifetime. That is, the predicted SLAs may
become invalid after some time.  
As part of this paper, we develop a model for detecting such SLA invalidations. 
We use this model to investigate
the effective lifetime of Cerebro predictions. When such changes occur,
Cerebro can be reinvoked to establish new SLAs for any deployed web API.  %Alternatively,
%the Cerebro PaaS monitoring mechanism can be used to identify potential SLA violations
%so that they can be avoided via mechanisms such 
%as web API rate limiting and platform resource acquisition.

We have implemented Cerebro for both the Google App Engine public PaaS, and 
the AppScale private PaaS. Given its modular design and this experience, 
we believe that Cerebro can be easily integrated into any PaaS system.
We use our prototype implementation to evaluate the accuracy of Cerebro 
as well as the tightness
of the bounds it predicts (i.e. the difference between the predictions and 
the actual API execution times). To this end, we carry out a range of experiments
 using App Engine applications that are available as open source.  

We also detail the duration over which 
these predictions hold in both GAE and AppScale.  
We find that Cerebro generates correct SLAs (predictions that meet or exceed their
probabilistic guarantees), and that these SLAs are valid over time periods ranging
from 1.4 hours to more than 24 hours.  
We also find that the high variability of performance in public PaaS clouds due to their multi-tenancy
and massive scale requires that Cerebro be more conservative in its predictions 
to achieve the desired level of correctness. In comparison, Cerebro is able to make
much tighter SLA predictions for web APIs hosted in private, single tenant clouds.

Because Cerebro provides this 
analysis statically it imposes no run-time overhead on the applications
themselves. It requires no run-time instrumentation of application code,
and it does not require any performance testing of the web APIs.
Furthermore, because the PaaS is scalable and SDK monitoring data is 
shared across all Cerebro executions, the continuous monitoring of the
cloud SDK generates no discernible load on the cloud platform.
Thus we believe Cerebro is suitable for highly scalable cloud
settings.

Finally, we have developed Cerebro for use with EAGER (\textbf{E}nforced
\textbf{A}PI \textbf{G}overnance \textbf{E}ngine for
\textbf{R}EST)~\cite{eager-fop15} --
an API governance system for PaaS clouds. EAGER attempts to enforce
governance policies at the deployment-time of cloud applications. These governance
policies are specified by cloud administrators to ensure the reliable
operation of the cloud and the deployed applications. PaaS
platforms include an application deployment phase during which the platform provisions
resources for the application, installs the application components, and
configures them to use the cloud SDKs. EAGER injects a policy checking and
enforcement step into this deployment workflow so that only applications that
are compliant with respect to site-specific policies are successfully deployed. 
%Because Cerebro requires no application instrumentation and because the call
%path analysis takes place off-line, it can be used by EAGER to implement
%policies governing response-time SLAs.  That is, 
Cerebro allows
PaaS administrators to define
EAGER policies that allow an application to be deployed \textit{only} when its
web APIs meet a pre-determined (or pre-negotiated) SLA target, and to be
notified by the platform when such SLAs require renegotiation.

%web API development, 
%it precludes the need for deployment, load testing, and
%instrumentation of the web APIs being analyzed.

We structure the rest of this paper as follows.
We first characterize the domain of 
PaaS-hosted web APIs for GAE and AppScale 
in Section~\ref{sec:approach}.   
We then present the design of Cerebro in Section~\ref{sec:design}
and overview our software architecture and prototype implementation.
Next, we
present our empirical evaluation of Cerebro in 
Section~\ref{sec:results}.
Finally,  we discuss related work (Section~\ref{sec:related_work}) and 
conclude (Section~\ref{sec:conclusions}).
