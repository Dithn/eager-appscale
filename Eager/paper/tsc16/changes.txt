(1) It is claimed that the built-in monitoring framework Roots does not add a significant overhead to the applications. It is suggested to provide more quantitative results in details. Same suggestions to the “near real time” and “nearly 100%”.

    Explaind near realtime under results section. Nearly 100% claim is implicit from our results, since we haven't encountered any false positives.

(2) In table 1, the number of anomalies detected is shown. What are the meanings of the datastore and user management? How could tell the accuracy of the detection of Roots?

    These are various services in the PaaS that we inject faults to. This is mentioned in multiple places in the paper. Updated table header to be more clear. Accuracy
    is gauged by checking if all the injected faults are detected by Roots (which it clearly does). If additional anomalies are detected, we verify them manually.

(3) In figure 4-7, the fault injection and anomaly detection are shown in a timely way. Could you authors provide more numeric data about the delay of the detection? Is there any work to compare with the speed performance?

    Clarified under results section. All anomalies are detected within 2-5 minutes of the start of fault injection.
 
(4) It is claimed that Roots collect data using asynchronous tasks, in order to avoid degrading the performance of the applications. In this case, could the authors discuss about how to make the detection in real time or nearly real time?

    Clarified under architecture section. Reporting operations are run frequently enough to retain near realtime properties. 

(5) The anomaly detectors and anomaly handlers are with fixed-sized sliding windows and the memory has a strict upper bound. If there are a lot of data to collect and analysis, how to provide real time detection?

    Anomaly detectors always work with a bounded data set, and can be guaranteed to run fast. Any additional heavy computations can be performed offline. This is
    what we currently do with the bottleneck identification. The expensive linear regressions are run offline, after an anomaly has been detected in near realtime.

1. The title! The balance between the content and the title is bad. The content is aggressive, while the title is so limited.
1.1 Most content is about monitoring, data collection, and analysis of Roots. Bottleneck identification is just a function or usage of the Roots.
1.2 Why just web applications?  Roots has collected data even from infrastructure level, and so technically Roots can work for any applications and even the Cloud OS.
1.3 The author should either find a proper title or narrow down the content.

2. The abstract! This abstract is too abstract. From the abstract, nobody can capture the soul of Roots.

3. Fig.1 shows a nearly full stack data collection. Good and ambitious! BUT, in practice, vendors and users are responsible to only their own realms. For example, users cannot touch as many infrastructure data as possible, while vendors usually do not push any impact on users' applications,  to which users are fully responsible. This is particularly true for IaaS. Although PaaS cloud vendors manage more, it is hardly for them to steal users' data such as access history.
3.1 Please clarify the working mode of Roots.
3.2 Please clarify Roots is intrusive or non-intrusive to PaaS Cloud, and why.

   Clarified under architecture section. Roots is operated by cloud providers as part of their cloud offerings. The insights gained by Roots can be communicated to both
   cloud administrators and application developers.

4. ElasticSearch can run in a distributed way. Fig. 3 implies a centralised solution. Why?

   Clarified that it can be deployed as a distributed service if needed. The diagram also states "ElasticSearch Cluster".

5. For monitoring, detection, analysis, and experiments, only some words, such like usage and latency, can be seen. What are the metrics, especially, the performance metrics, are recorded and considered in Roots?

   Existing prototype heavily relies on latency metrics, but other metrics can be considered in the future if necessary. Clarified under implementation section.

6.  Some metrics can be saturated. Usage is one of them and shows significant non-linear change. Moreover, monitored and sampled metrics data may not be time-sequenced due to many reasons in practical cloud platforms, such as AWS. Hence, in most cases, analysis has to be done on aggregation, and previous experience shows  that longer term aggregation results in better prediction, i.e. detection. This problem has not been mentioned at all.

   We do lot of our calculations on aggregation. Detectors and handlers do this implicitly. We allow configuring the aggregation time by controlling the analysis rate and
   window size. Added some text to implementation section to clarify.

7. Cloud monitoring and anomaly detection have not been well exploited in the related work. By simply searching in Google, we can find some latest ones, for example, http://ieeexplore.ieee.org/document/7389388/.

   We cannot go into a detailed discussion of all the recent related work due to space limitations. But I've added the WWW paper on Roots as a reference, which has
   a thorough and more up-to-date discussion on related work.

8. The experiments and the results are not well structured and not strongly supporting the claimed goodness of Roots.

   
