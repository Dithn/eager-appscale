%% complexity and convenience make diagnosing PaaS application performance hard
%% -- PaaS is opaque to programmers
%% -- PaaS services must operate at scale (asynchronous, distributed)
%% new PaaS service for performance diagnotics
%% -- can't require application instrumentation to be a PaaS service
%% -- must be fully automated
%% -- must be integrated with the runtime system 
%% this paper: anaomaly detection and root cause identification
%% -- anaomaly: change in performance that results in SLA violation
%% -- root cause: diagnosis of the reason for the anomaly as either a change
%%    in workload, increased latency an application component, or increased
%%    latency in a PaaS service and the identification of which one
%% -- Roots is more general with respect to detectors and handlers but we
%%    investigate anomaly detection and root cause identification defined
%%    above in this paper
%% diagnosis => after the fact
%% -- can use asynchronous, but must be able to time correlate
Cloud computing is a popular approach for deploying
applications at scale~\cite{Antonopoulos:2010:CCP:1855007,Pinheiro:2014:ACC:2618168.2618188}. 
This widespread adoption of cloud computing, particularly for deploying
web applications, is facilitated by ever-deepening software abstractions.
These abstractions elide the complexity necessary to enable scale, while
making application development easier and faster.
But they also obscure the runtime details of cloud applications, 
making the diagnosis of performance problems challenging.
Therefore, the rapid expansion of cloud technologies
combined with their increasing opacity has intensified the need 
for new techniques to
monitor applications deployed in cloud platforms~\cite{DaCunhaRodrigues:2016:MCC:2851613.2851619}. 

Application developers and cloud administrators generally wish to monitor 
application performance, detect anomalies, and identify bottlenecks. To obtain 
this level of operational insight into cloud-hosted applications, the cloud platforms must support 
data gathering and analysis capabilities that span the entire software stack of the cloud. 
However, most cloud technologies available
today do not provide adequate application monitoring support. Cloud administrators must therefore trust the
application developers to implement necessary instrumentation 
at the application level. This typically entails using third party, external monitoring software~\cite{newrelic,dynatrace,datadog},
which significantly increases the effort and financial cost of maintaining applications.
Developers must also ensure
that their instrumentation is both correct, and does not degrade 
application performance.  Nevertheless, since the applications depend on extant
cloud services (e.g. scalable database services, 
scalable in-memory caching services, etc.) that are performance opaque, it is
often difficult, if not impossible to diagnose the ``root cause'' of a performance problem
using such extrinsic forms of monitoring.

Further compounding the performance
diagnosis problem, today's cloud platforms are very 
large and complex~\cite{DaCunhaRodrigues:2016:MCC:2851613.2851619,Ibidunmoye:2015:PAD:2808687.2791120}. 
They are
comprised of many layers, where each layer may consist of many interacting components.
Therefore when a performance anomaly manifests in a user application, it is
often challenging
to determine the exact layer or the component of the cloud platform that may be responsible for it. 
Facilitating this level of comprehensive root cause analysis requires
both data collection at different layers of the cloud, and mechanisms for correlating 
the events recorded at different layers. 

Moreover, performance monitoring for cloud applications must be customizable. Different
applications have different monitoring requirements in terms of data gathering frequency (sampling rate), 
length of the history to consider when performing statistical analysis (sample size), and the performance 
SLOs (service level objectives~\cite{Keller:2003:WFS:635430.635442}) that govern the application.
Cloud monitoring should be able to facilitate these diverse requirements on a
per-application basis.
Designing such customizable and extensible performance
monitoring frameworks that are built into the cloud platforms is a novel and challenging undertaking.

To address these challenges, we develop 
a full-stack application performance
monitor (APM) called \textit{Roots}~\cite{Jayathilaka:2017:PMR:3038912.3052649}, as a cloud Platform-as-a-service (PaaS) extension.
PaaS clouds provide a very high level of abstraction that hides most of the details concerning application
runtime. They also provide a set of managed services, which developers compose into applications.
To be able to correlate application activity with cloud platform events,
we design Roots as another managed service built into the PaaS cloud. 
This way Roots can collect data
directly from the internal service implementations of the PaaS and
avoid instrumenting application code.

Prior work outlines several key requirements for cloud APMs~\cite{DaCunhaRodrigues:2016:MCC:2851613.2851619,Ibidunmoye:2015:PAD:2808687.2791120},
which we incorporate into Roots.  They are
\begin{LaTeXdescription}
\item[Scalability] Roots is lightweight, and does not cause any noticeable overhead in 
application performance. It puts strict upper bounds on the data kept in memory. 
The persistent data is accessed on demand, and can be removed after their usefulness has expired.
\item[Multitenancy] Roots facilitates configuring monitoring policies at the granularity of individual applications.
Users can employ different statistical analysis methods to process the monitoring data in ways that are 
most suitable for their applications.
\item[Complex application architecture] Roots collects data from the entire cloud stack 
(load balancers, app servers, built-in PaaS services etc.). It correlates data gathered
from different parts of the cloud platform, and performs systemwide bottleneck identification.
\item[Dynamic resource management] Cloud platforms are dynamic in terms of their magnitude 
and topology. Roots captures performance events of applications by augmenting 
the key components of the cloud platform. When new processes/components become active
in the cloud platform, they inherit the same augmentations, and start reporting to Roots automatically.
\item[Autonomy] Roots detects performance anomalies online without manual intervention.
When Roots detects a problem, it attempts to automatically identify the root cause by analyzing
available workload and service invocation data.
\vspace{-0.05in}
\end{LaTeXdescription}

Roots collects data from the logs and the interfaces of internal PaaS components.
In addition to high-level metrics like request throughput
and latency, 
Roots also measures PaaS service invocations and their duration.
It uses batch operations and asynchronous 
communication to record events in a manner that does not substantively
increase request latency.

When Roots detects a performance anomaly in an application, it attempts to recover the
root cause of the anomaly by analyzing the workload data
and the performance of the internal PaaS services on which the 
application depends.
Roots first determines if the detected anomaly was most likely caused by a change in the
application workload (e.g. a sudden spike in the number of client requests), or by an internal
bottleneck in the cloud platform (e.g. a slow database query). 
For the latter,
Roots employs a statistical bottleneck identification method 
that  combines quantile analysis, change point detection,
and linear regression to identify the root cause of the bottleneck.

Finally, we devise a mechanism for Roots that distinguishes
between different paths of execution in the application (control flows).
Our approach does not require static analysis bur instead uses the 
runtime data collected by Roots. This mechanism calculates the 
proportion of user requests processed by each path and uses it to 
characterize the workload
of an application (e.g. read-heavy vs write-heavy workload 
in a data management
application). Using this approach, Roots is able to
detect when application workloads change.

We build a working prototype of 
Roots using the AppScale~\cite{6488671} open source PaaS. We evaluate the feasibility and the 
efficacy of Roots by conducting a series of empirical trials using our prototype. 
We also show that our approach for identifying performance bottlenecks
in PaaS clouds, produces accurate results nearly 100\% of the time. 
We also demonstrate that Roots does not add a significant performance overhead
to the applications, and that it scales well to monitor tens of thousands
of applications concurrently.

%We make the following contributions with this paper:
%\begin{itemize}
%\item We describe the architecture of Roots as an intrinsic PaaS
%service, which works automatically without requiring or depending upon
%application instrumentation.
%\item We describe a statistical methodology for determining when an
%application is experiencing a performance anomaly, and identifying the 
%workload change or the application component that is responsible for the anomaly.
%\item We present a mechanism for identifying the execution paths of an
%application via the runtime data gathered from it, and characterizing
%the application workload by computing the proportion of requests handled 
%by each path.
%\item We demonstrate the effectiveness of the approach using a working PaaS
%prototype.
%\end{itemize}
%}

%Rest of this paper is organized as follows.
%Section~\ref{sec:background} sets the stage for introducing Roots by describing the domain of 
%PaaS clouds and discussing performance monitoring fundamentals. Section~\ref{sec:arch} 
%details the high level architecture of Roots along with the motivation behind our design choices.
%Section~\ref{sec:impl} describes our implementation of Roots, with a strong emphasis on our
%solution to the bottleneck identification problem in PaaS clouds. Section~\ref{sec:results} presents our
%experimental results. Then we discuss some related work and conclude. 
