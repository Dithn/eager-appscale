This widespread adoption of cloud computing, particularly for deploying
web applications, is facilitated by ever-deepening software abstractions.
These abstractions elide the complexity necessary to enable scale, while
making application development easier and faster.
But they also obscure the runtime details of cloud applications, 
making the diagnosis of performance problems challenging. 
Therefore, the rapid expansion of cloud technologies
combined with their increasing opacity has intensified the need 
for new techniques to
monitor applications deployed in cloud platforms~\cite{DaCunhaRodrigues:2016:MCC:2851613.2851619}. 

Timely detection
of performance problems and SLO violations, and the ability to identify the root causes
of such issues are critical elements of governance.
Application developers and cloud administrators generally wish to monitor 
application performance, detect anomalies, and identify bottlenecks. To obtain 
this level of operational insight into cloud-hosted applications, 
and facilitate governance, the cloud platforms must support 
data gathering and analysis capabilities that span the entire software stack of the cloud. 
However, most cloud technologies available
today do not provide adequate application monitoring support. 
Cloud administrators must therefore trust the
application developers to implement necessary instrumentation 
at the application level. This typically entails using third party, external monitoring software~\cite{newrelic,dynatrace,datadog},
which significantly increases the effort and financial cost of maintaining applications.
Developers must also ensure
that their instrumentation is both correct, and does not degrade 
application performance.  Nevertheless, since the applications depend on extant
cloud services (e.g. scalable database services, 
scalable in-memory caching, etc.) that are performance opaque, it is
often difficult, if not impossible to diagnose the root cause of a performance problem
using such extrinsic forms of monitoring.

Further compounding the performance
diagnosis problem, today's cloud platforms are very 
large and complex~\cite{DaCunhaRodrigues:2016:MCC:2851613.2851619,Ibidunmoye:2015:PAD:2808687.2791120}. 
They are
comprised of many layers, where each layer may consist of many interacting components.
Therefore when a performance anomaly manifests in a user application, it is
often challenging
to determine the exact layer or the component of the cloud platform that may be responsible for it. 
Facilitating this level of comprehensive root cause analysis requires
both data collection at different layers of the cloud, and mechanisms for correlating 
the events recorded at different layers. 

Moreover, performance monitoring for cloud applications needs to be highly customizable. Different
applications have different monitoring requirements in terms of data gathering frequency (sampling rate), 
length of the history to consider when performing statistical analysis (sample size), and the performance 
SLOs (service level objectives~\cite{Keller:2003:WFS:635430.635442}) and policies that govern the application.
Cloud monitoring should be able to facilitate these diverse requirements on a
per-application basis.
Designing such customizable and extensible performance
monitoring frameworks that are built into the cloud platforms is a novel and challenging undertaking.

To address these needs, we present a full-stack application performance
monitor (APM) called \textit{Roots} that can be integrated
with a variety of cloud Platform-as-a-Service (PaaS) technologies. 
PaaS clouds provide a set of managed services, which developers compose into applications.
To be able to correlate application activity with cloud platform events,
we design Roots as another managed service built into the PaaS cloud. 
Therefore it operates at the same level as the other services offered by the cloud platform. 
This way Roots can collect data
directly from the internal service implementations of the cloud platform, thus gaining full visibility into all the 
inner workings of an application. It also enables Roots to operate fully automatically in the background, without
requiring instrumentation of application code. 

Previous work has outlined several key requirements that need to be considered when
designing a cloud monitoring system~\cite{DaCunhaRodrigues:2016:MCC:2851613.2851619,Ibidunmoye:2015:PAD:2808687.2791120}. 
We incorporate many of these features into our design:
\begin{description}
\item[Scalability] Roots is lightweight, and does not cause any noticeable overhead in 
application performance. It puts strict upper bounds on the data kept in memory. 
The persistent data is accessed on demand, and can be removed after their usefulness has expired.
\item[Multitenancy] Roots facilitates configuring monitoring policies at the granularity of individual applications.
Users can employ different statistical analysis methods to process the monitoring data in ways that are 
most suitable for their applications.
\item[Complex application architecture] Roots collects data from the entire cloud stack 
(load balancers, app servers, built-in PaaS services etc.). It correlates data gathered
from different parts of the cloud platform, and performs systemwide bottleneck identification.
\item[Dynamic resource management] Cloud platforms are dynamic in terms of their magnitude 
and topology. Roots captures performance events of applications by augmenting 
the key components of the cloud platform. When new processes/components become active
in the cloud platform, they inherit the same augmentations, and start reporting to Roots automatically.
\item[Autonomy] Roots detects performance anomalies online without manual intervention.
When Roots detects a problem, it attempts to automatically identify the root cause by analyzing
available workload and service invocation data.
\end{description}

Roots collects most of the data it requires by direct integration with various internal components 
of the cloud platform. In addition to high-level metrics like request throughput
and latency, Roots also records the internal PaaS service invocations made by applications,
and the latency of those calls. It uses batch operations and asynchronous 
communication to record events in a manner that does not substantively
increase request latency.

The previous two chapters present systems that perform the \textit{specification} (policies and SLOs)
and \textit{enforcement} functions of governance.
Roots also performs an important function associated with automated governance -- \textit{monitoring}.
It is designed to monitor cloud-hosted web
applications for SLO violations, and any other deviations from specified or expected behavior. 
Roots flags such issues as anomalies, and notifies cloud administrators in near real time.
Also, when Roots detects an anomaly in an application, it attempts to uncover the
root cause of the anomaly by analyzing the workload data,
and the performance of the internal PaaS services the application depends on. 
Roots can determine if the detected anomaly was caused by a change in the
application workload (e.g. a sudden spike in the number of client requests), or an internal
bottleneck in the cloud platform (e.g. a slow database query). To this end we propose
a statistical bottleneck identification method for PaaS clouds. 
It uses a combination of quantile analysis, change point detection
and linear regression to perform root cause analysis. 

Using Roots we also devise a mechanism to identify different paths of execution in
an application -- i.e. different paths in the application's control flow graph. 
Our approach does not require static analysis, and instead uses the 
runtime data collected by Roots. This mechanism also calculates the proportion of 
user requests processed by each path, which is used to characterize the workload
of an application (e.g. read-heavy vs write-heavy workload in a data management
application). Based on that, Roots monitors for characteristic changes in the application
workload.

We build a working prototype of 
Roots using the AppScale~\cite{6488671} open source PaaS. We evaluate the feasibility and the 
efficacy of Roots by conducting a series of empirical trials using our prototype. 
We also show that our approach for identifying performance bottlenecks
in PaaS clouds, produces accurate results nearly 100\% of the time. 
We also demonstrate that Roots does not add a significant performance overhead
to the applications, and that it scales well to monitor tens of thousands
of applications concurrently.

We discuss the following contributions in this chapter:
\begin{itemize}
\item We describe the architecture of Roots as an intrinsic PaaS
service, which works automatically without requiring or depending upon
application instrumentation.
\item We describe a statistical methodology for determining when an
application is experiencing a performance anomaly, and identifying the 
workload change or the application component that is responsible for the anomaly.
\item We present a mechanism for identifying the execution paths of an
application via the runtime data gathered from it, and characterizing
the application workload by computing the proportion of requests handled 
by each path.
\item We demonstrate the effectiveness of the approach using a working PaaS
prototype.
\end{itemize}