PaaS applications rely on a set of managed, scalable services 
offered by the underlying cloud platform. We refer to these services as 
PaaS kernel services. PaaS clouds like Google
App Engine~\cite{gae} and Microsoft Azure~\cite{azure} export
the kernel services via a well-defined set of APIs, that are collectively
referred to as the PaaS ``software development kit'' (SDK). The application
servers provide the linkage between application code and the PaaS kernel
services. A set of front-end servers expose web application entry points, and
provide load-balancing for HTTP/S clients invoking the applications.

By providing most of the functionality that applications require via
kernel services, the PaaS model
significantly increases programmer productivity.
However, a downside of this approach is that these features also hide
the performance details of PaaS applications. 
Since the applications spend most of their time executing kernel 
services~\cite{Jayathilaka:2015:RTS:2806777.2806842},
it is challenging for the developers to diagnose performance issues given the
opacity of the cloud platform's internal implementation. 

One way to circumvent this 
problem is to instrument application code~\cite{newrelic,datadog,dynatrace}, 
and continuously monitor the time taken by various
parts of the application. But such application-level instrumentation is tedious, and
error prone thereby misleading those attempting to diagnose problems.
Moreover, the instrumentation code may slow down or alter the application's
performance. 
In contrast, implementing data collection and analysis as a kernel service 
built into the PaaS cloud allows 
performance diagnosis to be a ``curated'' service that is 
reliably managed by the cloud platform.

In order to maintain a satisfactory level of user experience and adhere to any previously
agreed upon performance SLOs, application developers and cloud administrators wish
to detect performance anomalies as soon as they occur. When detected, they
must perform root cause analysis to identify the cause of the anomaly, and take some
corrective and/or preventive action. 
This diagnosis usually occurs as a two step process. First, one must determine
whether the anomaly was caused by a change in the workload (e.g. a sudden 
increase in the number of client requests). If that is the case,
the resolution typically involves allocating more resources to the application or spawning
more instances of the application for load balancing purposes. If the anomaly cannot be 
attributed to a workload change, one must go another step to find the bottleneck component
that has given rise to the issue at hand.

%Detecting performance anomalies
%requires continuous monitoring of application performance which could be tedious with
%cloud platforms in use today. It is even more challenging to perform root cause analysis
%due to the complexity and the blackbox nature of the cloud platforms. 

%Note that there are several
%third party cloud monitoring solutions available today which provide some performance
%anomaly detection support~\cite{newrelic,datadog,dynatrace}. 
%However, they require additional configuration and code instrumentation, are expensive
%and cannot support root cause analysis across the entire cloud stack since they do not
%have visibility into all components of the cloud platform.
