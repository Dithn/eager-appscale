Cerebro is a system that predicts response time 
bounds for web APIs deployed in PaaS clouds.  It does so via 
static analysis to extract the sequence of cloud SDK 
calls (i.e. PaaS kernel invocations) made by a given web API code combined with
historical performance measurements of cloud SDK calls. 
Cerebro employs QBETS, a non-parametric time series analysis and 
forecasting method, to analyze
cloud SDK performance data, and predict bounds
on response time that can be used as statistical ``guarantees'' with
associated guarantee probabilities.
Cerebro is intended for use during development and 
deployment phases of a web API, and 
precludes the need for continuous performance testing of the API code. 
Further, it does not interfere with run-time operation (i.e. it requires
no application instrumentation) making it scalable.

We have implemented a prototype of Cerebro for Google App Engine public PaaS,
and AppScale private PaaS,
and evaluate it using a set of representative
and open source web applications developed by others.  
Our findings indicate that the prototype can determine response time levels
that correspond to specific target SLOs.  These predictions are also durable,
with average validity times varying between one day and several weeks.
We also find that API consumers do not experience SLO changes often, and the maximum
number of times an API consumer encounters an SLO change over a period of 112 days is six.
Overall, this work shows that automatic definition of response-time SLOs for web APIs is practically
viable in real world cloud settings, and API consumer timeframes.

We use Cerebro to predict the 95th percentile of the API operation response time. 
We find that:
\begin{itemize}
\item Cerebro achieves the desired correctness goal of 95\% for all the applications in both cloud environments.
\item Cerebro generates tight predictions (i.e.
the predictions are similar to measured values) for most web APIs.  Because
some operations and PaaS systems exhibit more variability in cloud SDK response
time, 
Cerebro must be conservative in some cases, and produce predictions that are less tight
to meet its correctness guarantees.  
\item Cerebro requires a ``warm up'' period of up to 200 minutes to produce trustworthy 
predictions. Since PaaS systems are designed to run continuously, this is not an issue in practice. 
\item We can use a simple yet administratively useful model to identify when an 
SLO becomes invalid to compute
prediction validity durations for Cerebro.  The average duration of a valid
Cerebro prediction is between 24 and 72 hours,
and 95\% of the time this duration is at least 
1.41 hours for App Engine and 1.95 hours for AppScale.
\end{itemize}

Cerebro facilitates reasoning about the performance of web APIs deployed
in a cloud environment, a capability that is crucial for implementing several
automated governance features. The bounds predicted by Cerebro can be used to
enforce policies regarding the performance level expected from cloud-hosted
web applications. They can be used as SLOs, which in turns can be used to
formulate SLAs that are advertised to API clients. They can also be used as
thresholds when implementing application performance monitoring (APM),
subject of the next chapter.

In the current design, Cerebro's cloud SDK monitoring agent only monitors 
a predefined set of cloud SDK operations. In our future work we wish 
to explore the possibility of making this component more dynamic,
so that it automatically learns what operations to benchmark from the web APIs 
deployed in the cloud. This also includes learning the size and the form of the datasets
that cloud SDK invocations operate on, so that Cerebro can acquire more realistic
benchmarking data. We also plan to investigate further how to better
handle data-dependent loops (iterative datastore reads) for different workloads. We are interested
in exploring the ways in which we can handle API codes with unpredictable execution patterns (e.g.
loops based on a random number), even though such cases are quite rare in the applications we
have looked at so far.
Further, we plan
to integrate Cerebro with EAGER, our API governance system 
and policy engine for PaaS clouds, so 
that PaaS administrators can enforce SLO-related policies on web APIs at deployment-time.
Such a system will make it possible to prevent any API that 
does not adhere to the organizational performance
standards from being deployed in the production cloud environment. It can 
also enforce policies that prevent applications from taking dependencies on APIs
that are not up to the expected performance standards.