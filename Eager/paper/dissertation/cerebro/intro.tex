Cloud-hosted web applications are deployed and used as web services. 
They enable a level of service reuse that both expedites
and simplifies the development of new client applications.
Despite the many benefits, reusing existing services also has pitfalls.  
In particular, new client applications become dependent on the 
services they compose.  These dependencies
impact correctness, performance, and availability of the composite 
applications, for which the ``top level'' developer is often held accountable.  
Compounding the situation, the underlying services can and do change over time
while their APIs remain stable,
unbeknownst to the developers that programmatically access them.
Unfortunately, there is a dearth of tools that help developers reason about these 
dependencies throughout an application's 
life cycle (i.e. development, deployment, and run-time).  Without such tools, 
programmers must adopt extensive, continuous, and costly, testing and profiling methods
to understand the performance impact on their applications
that results from the increasingly complex collection of
services that they depend on.

To address this issue, we present Cerebro, a new approach that 
predicts bounds on the response time performance of web
APIs exported by applications that are hosted in a PaaS cloud.
The goal of Cerebro is to allow 
a PaaS administrator to determine what response time service level objective (SLO) can be
fulfilled by each web API operation exported by the applications
hosted in the PaaS. 

An ``SLO'' specifies the minimum service level promised by the
service provider regarding some non-functional property of the service such as
its availability or performance (response time). 
Such SLOs are explicitly
stated by the service provider, and are typically associated with a correctness probability,
which can be described as the likelihood the service will meet the promised
minimum service level. An availability SLO that follows this notion
takes the form: ``the service will be available $p\%$ of the time''.
Here the value $p\%$ is the correctness probability of the SLO. Similarly, a
response time SLO would take the form of the statement: ``the service will respond 
under $Q$ milliseconds, $p\%$ of the time. Naturally, $p$ should be a value close
to $100$, for this type of SLOs to be useful in practice.

In a corporate setting, SLOs are used to form service level agreements (SLAs), 
formal contracts that govern the service provider-consumer relationship~\cite{Keller:2003:WFS:635430.635442}.
They consist of SLOs, and the clauses that describe what happens if the
service fails to meet those SLOs (for example, if the
service is only available $p^\prime\%$ of the time, where $p^\prime < p$). This
typically boils down to service provider paying some penalty (a refund), or 
providing some form of free service credits for the users. We do not consider such
legal and social obligations of an SLA in this work, and simply focus on the
minimum service levels (i.e. SLOs), and the associated correctness probabilities, since those
are the things that matter from a performance and capacity planning point of view
of an application. 

Currently, cloud computing systems such as Amazon Web Services
(AWS)~\cite{amazon-aws-web} and
Google App Engine (GAE)~\cite{gae} advertise SLOs specifying the fraction of
availability over a fixed time period (i.e. uptime) for their services.
However, they do not provide SLOs
that state minimum levels of performance.
In contrast, Cerebro facilitates auto-generating performance SLOs for cloud-hosted
web APIs in a way that is scalable.
Cerebro uses a combination of static analysis of the hosted web APIs, 
and runtime monitoring of the PaaS kernel services
to determine what minimum statistical guarantee can be made regarding an
API's response time,
with a target probability specified by a PaaS administrator. These calculated
SLOs enable developers to
reason about the performance of the client applications that consume the cloud-hosted
web APIs. They can also be used to negotiate SLAs regarding the performance of
cloud-hosted web applications.
Moreover, predicted SLOs are useful as baselines or thresholds when monitoring APIs 
for consistent performance -- a feature that is useful for both API providers
as well as consumers. Collectively, Cerebro and the SLOs predicted by it 
enable implementing a number of automated governance scenarios involving
policy enforcement and application performance monitoring, in ways that
were not possible before.

Cerebro generates response time SLOs for APIs exported by a web
application
developed using the kernel services available within the PaaS.  For brevity, in this work
we will use the
term \textit{web API} to refer to a web-accessible API exported by an
application hosted on a PaaS platform. Further, 
we will use the term \textit{kernel services} to refer to the services that are 
maintained as part of the PaaS and
available to all hosted applications. This terminology enables us to
differentiate the internal services of the PaaS from the 
APIs exported by the deployed applications.   
For example, an application hosted in Google App Engine might export one or
more web APIs to its users while leveraging the internal 
datastore kernel service that is available as part of the Google App Engine PaaS.

Cerebro uses static analysis to identify the PaaS kernel invocations
that dominate the response time of web APIs. By surveying a collection of
web applications developed for a PaaS cloud, we show
that such applications indeed spend majority of their execution time on PaaS kernel
invocations. Further, they do not have many branches and loops, which
makes them amenable to static analysis (section~\ref{sec:cerebro_approach}). Independently,
Cerebro also maintains a running history of response time 
performance for PaaS kernel services.  It uses
QBETS~\cite{Nurmi:2007:QQB:1791551.1791556} -- a forecasting methodology
we have developed in prior work for predicting bounds on ``ill behaved''
univariate time series -- to predict response time bounds on each PaaS kernel
invocation made by the application.  It combines these predictions dynamically
for each static program path through a web API operation,
and returns the ``worst-case''
upper bound on the time necessary to 
complete the operation.

Because service implementations and platform behavior under load change over time,
Cerebro's predictions necessarily have a lifetime. That is, the predicted SLOs may
become invalid after some time.  
As part of this chapter, we develop a model for detecting such SLO invalidations. 
We use this model to investigate
the effective lifetime of Cerebro predictions. When such changes occur,
Cerebro can be reinvoked to establish new SLOs for any deployed web API.  

We have implemented Cerebro for both the Google App Engine public PaaS, and 
the AppScale private PaaS. Given its modular design and this experience, 
we believe that Cerebro can be easily integrated into any PaaS system.
We use our prototype implementation to evaluate the accuracy of Cerebro, 
as well as the tightness
of the bounds it predicts (i.e. the difference between the predictions and 
the actual API execution times). To this end, we carry out a range of experiments
 using App Engine applications that are available as open source.  

We also detail the duration over which 
Cerebro predictions hold in both GAE and AppScale.  
We find that Cerebro generates correct SLOs (predictions that meet or exceed their
probabilistic guarantees), and that these SLOs are valid over time periods ranging
from 1.4 hours to several weeks.  
We also find that the high variability of performance in public PaaS clouds due to their multi-tenancy
and massive scale requires that Cerebro be more conservative in its predictions 
to achieve the desired level of correctness. In comparison, Cerebro is able to make
much tighter SLO predictions for web APIs hosted in private, single tenant clouds.

Because Cerebro provides this 
analysis statically it imposes no run-time overhead on the applications
themselves. It requires no run-time instrumentation of application code,
and it does not require any performance testing of the web APIs.
Furthermore, because the PaaS is scalable and platform monitoring data is 
shared across all Cerebro executions, the continuous monitoring of the
kernel services generates no discernible load on the cloud platform.
Thus we believe Cerebro is suitable for highly scalable cloud
settings.

Finally, we have developed Cerebro for use with EAGER (\textbf{E}nforced
\textbf{A}PI \textbf{G}overnance \textbf{E}ngine for
\textbf{R}EST)~\cite{eager-fop15} --
an API governance system for PaaS clouds. EAGER attempts to enforce
governance policies at the deployment-time of cloud applications. These governance
policies are specified by cloud administrators to ensure the reliable
operation of the cloud and the deployed applications. PaaS
platforms include an application deployment phase during which the platform provisions
resources for the application, installs the application components, and
configures them to use the kernel services. EAGER injects a policy checking and
enforcement step into this deployment workflow, so that only applications that
are compliant with respect to site-specific policies are successfully deployed. 
Cerebro allows PaaS administrators to define
EAGER policies that allow an application to be deployed \textit{only} when its
web APIs meet a pre-determined SLO target, and to be
notified by the platform when such SLOs require revision.

We structure the rest of this chapter as follows.
We first characterize the domain of 
PaaS-hosted web APIs for GAE and AppScale 
in Section~\ref{sec:cerebro_approach}.   
We then present the design of Cerebro in section~\ref{sec:cerebro_design}
and overview our software architecture and prototype implementation.
Next, we
present our empirical evaluation of Cerebro in 
section~\ref{sec:cerebro_results}.
Finally,  we discuss related work (Section~\ref{sec:cerebro_related_work}) and 
conclude (Section~\ref{sec:cerebro_conclusions}).