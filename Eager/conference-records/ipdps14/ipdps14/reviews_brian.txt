@paper: 1570044771
### "Towards a high level programming paradigm to deploy e-science applications with dynamic workflows on large scale distributed systems"
@type: review
** Key Contributions:
Please describe the key contributions of the paper or lack thereof. Your comments should be specific and justify your overall recommendation.
(viewable by reviewers who have completed their review, conference chairs or paper author)
@T1
-replace with one or more lines of review text-


The authors build an infrastructure to enable different two e-science applications utilize cloud and grid computing computational resources: MetaPIGA and NeuroWeb. They elucidate lessons learned and show the structure of their programming paradigm focusing on the "Dynamic Workflows", which they define as an iterative master/slave algorithm with a variable number of jobs per iteration and a convergence criteria.  In the case of NeuroWeb, they present a Dynamic Workflow that samples a persistently running process on each of the workers. Specifically, they present and API for integration of e-science applications with their prior works, the XWCH software.  Their method of integration is by extending a predefined java classes to define functions such as "isConverged()", "getInputParameter()", providing the logic necessary for the specific application.

The objective of their work is to use these programming paradigms to enable non-specialized scientists to integrate e-science applications with cloud and grid platforms. By abstracting away technical details into their middleware platform they reduced the parallel and cloud computing specific programming knowledge necessary to use these advanced computing platforms.

They show the technical details involved in the integration of the two example e-science application, giving examples of code and process flow.  Finally, they compare the performance of their programming model across different clouds (EC2, Azure, and a hybrid with both) and versus another concurrent programming tool GC3Pie where they show a slight improvement is scaling with respect to number of jobs.


@@

** Suggestions for Improvement:
Additional comments and suggestions for improvement in the technical content or the presentation. Please be as detailed and constructive as you can be.
(viewable by reviewers who have completed their review, track chairs, conference chairs or paper author)
@T2
-replace with one or more lines of review text-

The authors note that distributed computing is an increasingly complex field with the following line: "Non IT scientists are increasingly “forced” to master high programming skills instead of focusing on their science."  However, while the integration API presented in the paper is much simpler than direct parallel programming it presents a programming paradigm complex enough to be a challenge for a Non-IT scientist.  

In the comparison of performance across clouds, it would be helpful to understand where difference in speed is coming from.  Is it fully explained by the difference in processor speed, or are their bandwidth or I/O issues as well?

Typos:
Page 1, column 1, paragraph 1: "Non IT scientists" should be "Non-IT scientists"
	"master high programming skills" should be "master high-level programming skills"
Page 3, column 1, paragraph 1: "embarrassingly parallels" should be "embarrassingly parallel"
Note:  There are several places where "non " should be replaced by "non-".  This is inconsistent throughout the document.

@@

** Comments to TPC:
This section is specifically for comments which for some reason should NOT be sent to the authors.  Detail will be helpful here also.
(viewable by reviewers who have completed their review, track chairs or conference chairs)
@T3
This paper is well written and presents an interesting adaptation of software to distributed computing resources, however the impact is very low as the software target is already published and the workflow is already well known.
@@

** Award quality?:
Do you consider the paper a candidate for a best-paper award? If so, why? These comments will NOT be sent to authors.
(viewable by reviewers who have completed their review, track chairs or conference chairs)
@T4
Not Award quality.

@@

** Significance:
Assess the significance of the topic addressed in the paper.
(viewable by reviewers who have completed their review, conference chairs or paper author)
1:None; 2:Below average; 3:Above average; 4:Excellent
@S1= 2

** Originality/Novelty (of contribution):
How novel are the concepts presented in the paper?
(viewable by reviewers who have completed their review, conference chairs or paper author)
1:None; 2:Below average; 3:Above average; 4:Excellent
@S2= 2

** Technical Soundness:
How strong are the techniques and methodologies used in the paper?
(viewable by reviewers who have completed their review, conference chairs or paper author)
1:Poor; 2:Weak; 3:Strong; 4:Excellent
@S3= 3

** Quality of Presentation:
Assess both the ease of reading the paper and the extent to which it gets its contributions across.
(viewable by reviewers who have completed their review or conference chairs)
1:Poor; 2:Below average; 3:Above average; 4:Excellent
@S4= 3

** Expertise:
Please assess your expertise in the subject matter of the paper.
(viewable by reviewers who have completed their review or conference chairs)
1: Totally outside the subject matter; 2: Little familiarity with the subject matter; 3: Average knowledge on the subject matter ; 4: Knows the subject matter in a reasonable manner; 5: Expert in the subject matter
@S5= 5

** Overall Recommendation:
Your final rating should be consistent with your ratings on previous questions.
(viewable by reviewers who have completed their review, track chairs, conference chairs or paper author)
1:Definite Reject; 2:Reject; 3:Weak Reject; 4:Weak accept; 5:Accept; 6:Definite accept
@S6= 4

@end

